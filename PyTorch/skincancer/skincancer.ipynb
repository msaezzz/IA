{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df35b3aa-3ed5-49b6-a2e3-45099ce50c12",
   "metadata": {},
   "source": [
    "# Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c23412-122f-4fc7-88b0-b829e77d0cf3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a66f95-8c90-46eb-babf-7164435708c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import backward\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import ReLU\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1813ea9-c9e2-4c37-bb25-a21aba2e11a7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e718c5-8cc4-4e95-8b47-f9ade6893bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasando de Imagen a Tensor\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((224,224)),\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#Definiendo que es el Dataset y preparando el DataLoader\n",
    "directory = './DermMel'\n",
    "train_dataset = datasets.ImageFolder(root=\"./DermMel/train_sep\",transform=transform)\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_dataset = datasets.ImageFolder(root=\"./DermMel/test\",transform=transform)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac16858-577d-4eac-b6d6-7a5c5cf9ffe8",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0324792-c0c6-4b83-8b24-a6363bb372f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo del train dataset print(train_dataset[10][0][RGB,px,py])\n",
    "#Modelo\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Device: {device}')\n",
    "class MelanomaDetector(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__ ()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5,stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10816,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,2)\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        X = self.features(X)\n",
    "        return X\n",
    "\n",
    "#Instancia del modelo \n",
    "model = MelanomaDetector().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febde7a-05a8-4c41-a408-ee18faba6e3b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b974b730-a219-4adb-a3d6-e537892768d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0041\n",
      "Epoch [1/5], Loss: 0.0179\n",
      "Epoch [1/5], Loss: 0.0221\n",
      "Epoch [1/5], Loss: 0.0262\n",
      "Epoch [1/5], Loss: 0.0302\n",
      "Epoch [1/5], Loss: 0.0340\n",
      "Epoch [1/5], Loss: 0.0375\n",
      "Epoch [1/5], Loss: 0.0414\n",
      "Epoch [1/5], Loss: 0.0497\n",
      "Epoch [1/5], Loss: 0.0536\n",
      "Epoch [1/5], Loss: 0.0590\n",
      "Epoch [1/5], Loss: 0.0634\n",
      "Epoch [1/5], Loss: 0.0680\n",
      "Epoch [1/5], Loss: 0.0725\n",
      "Epoch [1/5], Loss: 0.0765\n",
      "Epoch [1/5], Loss: 0.0806\n",
      "Epoch [1/5], Loss: 0.0847\n",
      "Epoch [1/5], Loss: 0.0886\n",
      "Epoch [1/5], Loss: 0.0927\n",
      "Epoch [1/5], Loss: 0.0966\n",
      "Epoch [1/5], Loss: 0.1005\n",
      "Epoch [1/5], Loss: 0.1042\n",
      "Epoch [1/5], Loss: 0.1077\n",
      "Epoch [1/5], Loss: 0.1114\n",
      "Epoch [1/5], Loss: 0.1153\n",
      "Epoch [1/5], Loss: 0.1191\n",
      "Epoch [1/5], Loss: 0.1231\n",
      "Epoch [1/5], Loss: 0.1274\n",
      "Epoch [1/5], Loss: 0.1315\n",
      "Epoch [1/5], Loss: 0.1355\n",
      "Epoch [1/5], Loss: 0.1391\n",
      "Epoch [1/5], Loss: 0.1432\n",
      "Epoch [1/5], Loss: 0.1471\n",
      "Epoch [1/5], Loss: 0.1511\n",
      "Epoch [1/5], Loss: 0.1548\n",
      "Epoch [1/5], Loss: 0.1586\n",
      "Epoch [1/5], Loss: 0.1628\n",
      "Epoch [1/5], Loss: 0.1665\n",
      "Epoch [1/5], Loss: 0.1703\n",
      "Epoch [1/5], Loss: 0.1738\n",
      "Epoch [1/5], Loss: 0.1775\n",
      "Epoch [1/5], Loss: 0.1814\n",
      "Epoch [1/5], Loss: 0.1848\n",
      "Epoch [1/5], Loss: 0.1878\n",
      "Epoch [1/5], Loss: 0.1915\n",
      "Epoch [1/5], Loss: 0.1952\n",
      "Epoch [1/5], Loss: 0.1988\n",
      "Epoch [1/5], Loss: 0.2029\n",
      "Epoch [1/5], Loss: 0.2063\n",
      "Epoch [1/5], Loss: 0.2098\n",
      "Epoch [1/5], Loss: 0.2134\n",
      "Epoch [1/5], Loss: 0.2173\n",
      "Epoch [1/5], Loss: 0.2210\n",
      "Epoch [1/5], Loss: 0.2242\n",
      "Epoch [1/5], Loss: 0.2269\n",
      "Epoch [1/5], Loss: 0.2304\n",
      "Epoch [1/5], Loss: 0.2335\n",
      "Epoch [1/5], Loss: 0.2370\n",
      "Epoch [1/5], Loss: 0.2400\n",
      "Epoch [1/5], Loss: 0.2436\n",
      "Epoch [1/5], Loss: 0.2471\n",
      "Epoch [1/5], Loss: 0.2508\n",
      "Epoch [1/5], Loss: 0.2536\n",
      "Epoch [1/5], Loss: 0.2568\n",
      "Epoch [1/5], Loss: 0.2606\n",
      "Epoch [1/5], Loss: 0.2635\n",
      "Epoch [1/5], Loss: 0.2665\n",
      "Epoch [1/5], Loss: 0.2700\n",
      "Epoch [1/5], Loss: 0.2730\n",
      "Epoch [1/5], Loss: 0.2761\n",
      "Epoch [1/5], Loss: 0.2795\n",
      "Epoch [1/5], Loss: 0.2836\n",
      "Epoch [1/5], Loss: 0.2871\n",
      "Epoch [1/5], Loss: 0.2907\n",
      "Epoch [1/5], Loss: 0.2940\n",
      "Epoch [1/5], Loss: 0.2975\n",
      "Epoch [1/5], Loss: 0.3010\n",
      "Epoch [1/5], Loss: 0.3047\n",
      "Epoch [1/5], Loss: 0.3080\n",
      "Epoch [1/5], Loss: 0.3114\n",
      "Epoch [1/5], Loss: 0.3145\n",
      "Epoch [1/5], Loss: 0.3178\n",
      "Epoch [1/5], Loss: 0.3211\n",
      "Epoch [1/5], Loss: 0.3248\n",
      "Epoch [1/5], Loss: 0.3281\n",
      "Epoch [1/5], Loss: 0.3314\n",
      "Epoch [1/5], Loss: 0.3344\n",
      "Epoch [1/5], Loss: 0.3376\n",
      "Epoch [1/5], Loss: 0.3408\n",
      "Epoch [1/5], Loss: 0.3439\n",
      "Epoch [1/5], Loss: 0.3463\n",
      "Epoch [1/5], Loss: 0.3500\n",
      "Epoch [1/5], Loss: 0.3535\n",
      "Epoch [1/5], Loss: 0.3572\n",
      "Epoch [1/5], Loss: 0.3606\n",
      "Epoch [1/5], Loss: 0.3637\n",
      "Epoch [1/5], Loss: 0.3674\n",
      "Epoch [1/5], Loss: 0.3712\n",
      "Epoch [1/5], Loss: 0.3741\n",
      "Epoch [1/5], Loss: 0.3774\n",
      "Epoch [1/5], Loss: 0.3802\n",
      "Epoch [1/5], Loss: 0.3837\n",
      "Epoch [1/5], Loss: 0.3867\n",
      "Epoch [1/5], Loss: 0.3898\n",
      "Epoch [1/5], Loss: 0.3928\n",
      "Epoch [1/5], Loss: 0.3962\n",
      "Epoch [1/5], Loss: 0.3997\n",
      "Epoch [1/5], Loss: 0.4028\n",
      "Epoch [1/5], Loss: 0.4067\n",
      "Epoch [1/5], Loss: 0.4096\n",
      "Epoch [1/5], Loss: 0.4129\n",
      "Epoch [1/5], Loss: 0.4162\n",
      "Epoch [1/5], Loss: 0.4195\n",
      "Epoch [1/5], Loss: 0.4228\n",
      "Epoch [1/5], Loss: 0.4258\n",
      "Epoch [1/5], Loss: 0.4298\n",
      "Epoch [1/5], Loss: 0.4329\n",
      "Epoch [1/5], Loss: 0.4355\n",
      "Epoch [1/5], Loss: 0.4380\n",
      "Epoch [1/5], Loss: 0.4406\n",
      "Epoch [1/5], Loss: 0.4444\n",
      "Epoch [1/5], Loss: 0.4475\n",
      "Epoch [1/5], Loss: 0.4513\n",
      "Epoch [1/5], Loss: 0.4547\n",
      "Epoch [1/5], Loss: 0.4579\n",
      "Epoch [1/5], Loss: 0.4611\n",
      "Epoch [1/5], Loss: 0.4643\n",
      "Epoch [1/5], Loss: 0.4666\n",
      "Epoch [1/5], Loss: 0.4711\n",
      "Epoch [1/5], Loss: 0.4747\n",
      "Epoch [1/5], Loss: 0.4771\n",
      "Epoch [1/5], Loss: 0.4796\n",
      "Epoch [1/5], Loss: 0.4828\n",
      "Epoch [1/5], Loss: 0.4857\n",
      "Epoch [1/5], Loss: 0.4889\n",
      "Epoch [1/5], Loss: 0.4920\n",
      "Epoch [1/5], Loss: 0.4955\n",
      "Epoch [1/5], Loss: 0.4990\n",
      "Epoch [1/5], Loss: 0.5016\n",
      "Epoch [1/5], Loss: 0.5044\n",
      "Epoch [1/5], Loss: 0.5080\n",
      "Epoch [1/5], Loss: 0.5118\n",
      "Epoch [1/5], Loss: 0.5152\n",
      "Epoch [1/5], Loss: 0.5180\n",
      "Epoch [1/5], Loss: 0.5211\n",
      "Epoch [1/5], Loss: 0.5246\n",
      "Epoch [1/5], Loss: 0.5276\n",
      "Epoch [1/5], Loss: 0.5302\n",
      "Epoch [1/5], Loss: 0.5334\n",
      "Epoch [1/5], Loss: 0.5372\n",
      "Epoch [1/5], Loss: 0.5403\n",
      "Epoch [1/5], Loss: 0.5433\n",
      "Epoch [1/5], Loss: 0.5464\n",
      "Epoch [1/5], Loss: 0.5500\n",
      "Epoch [1/5], Loss: 0.5533\n",
      "Epoch [1/5], Loss: 0.5570\n",
      "Epoch [1/5], Loss: 0.5607\n",
      "Epoch [1/5], Loss: 0.5639\n",
      "Epoch [1/5], Loss: 0.5668\n",
      "Epoch [1/5], Loss: 0.5694\n",
      "Epoch [1/5], Loss: 0.5725\n",
      "Epoch [1/5], Loss: 0.5761\n",
      "Epoch [1/5], Loss: 0.5796\n",
      "Epoch [1/5], Loss: 0.5825\n",
      "Epoch [1/5], Loss: 0.5851\n",
      "Epoch [1/5], Loss: 0.5876\n",
      "Epoch [1/5], Loss: 0.5898\n",
      "Epoch [2/5], Loss: 0.0031\n",
      "Epoch [2/5], Loss: 0.0053\n",
      "Epoch [2/5], Loss: 0.0081\n",
      "Epoch [2/5], Loss: 0.0113\n",
      "Epoch [2/5], Loss: 0.0142\n",
      "Epoch [2/5], Loss: 0.0167\n",
      "Epoch [2/5], Loss: 0.0200\n",
      "Epoch [2/5], Loss: 0.0227\n",
      "Epoch [2/5], Loss: 0.0259\n",
      "Epoch [2/5], Loss: 0.0286\n",
      "Epoch [2/5], Loss: 0.0317\n",
      "Epoch [2/5], Loss: 0.0347\n",
      "Epoch [2/5], Loss: 0.0371\n",
      "Epoch [2/5], Loss: 0.0405\n",
      "Epoch [2/5], Loss: 0.0435\n",
      "Epoch [2/5], Loss: 0.0457\n",
      "Epoch [2/5], Loss: 0.0486\n",
      "Epoch [2/5], Loss: 0.0511\n",
      "Epoch [2/5], Loss: 0.0533\n",
      "Epoch [2/5], Loss: 0.0564\n",
      "Epoch [2/5], Loss: 0.0591\n",
      "Epoch [2/5], Loss: 0.0623\n",
      "Epoch [2/5], Loss: 0.0651\n",
      "Epoch [2/5], Loss: 0.0687\n",
      "Epoch [2/5], Loss: 0.0711\n",
      "Epoch [2/5], Loss: 0.0737\n",
      "Epoch [2/5], Loss: 0.0765\n",
      "Epoch [2/5], Loss: 0.0789\n",
      "Epoch [2/5], Loss: 0.0833\n",
      "Epoch [2/5], Loss: 0.0863\n",
      "Epoch [2/5], Loss: 0.0892\n",
      "Epoch [2/5], Loss: 0.0927\n",
      "Epoch [2/5], Loss: 0.0948\n",
      "Epoch [2/5], Loss: 0.0974\n",
      "Epoch [2/5], Loss: 0.1004\n",
      "Epoch [2/5], Loss: 0.1032\n",
      "Epoch [2/5], Loss: 0.1061\n",
      "Epoch [2/5], Loss: 0.1093\n",
      "Epoch [2/5], Loss: 0.1119\n",
      "Epoch [2/5], Loss: 0.1144\n",
      "Epoch [2/5], Loss: 0.1179\n",
      "Epoch [2/5], Loss: 0.1203\n",
      "Epoch [2/5], Loss: 0.1224\n",
      "Epoch [2/5], Loss: 0.1255\n",
      "Epoch [2/5], Loss: 0.1282\n",
      "Epoch [2/5], Loss: 0.1309\n",
      "Epoch [2/5], Loss: 0.1344\n",
      "Epoch [2/5], Loss: 0.1379\n",
      "Epoch [2/5], Loss: 0.1409\n",
      "Epoch [2/5], Loss: 0.1436\n",
      "Epoch [2/5], Loss: 0.1471\n",
      "Epoch [2/5], Loss: 0.1500\n",
      "Epoch [2/5], Loss: 0.1526\n",
      "Epoch [2/5], Loss: 0.1554\n",
      "Epoch [2/5], Loss: 0.1581\n",
      "Epoch [2/5], Loss: 0.1612\n",
      "Epoch [2/5], Loss: 0.1646\n",
      "Epoch [2/5], Loss: 0.1673\n",
      "Epoch [2/5], Loss: 0.1702\n",
      "Epoch [2/5], Loss: 0.1726\n",
      "Epoch [2/5], Loss: 0.1752\n",
      "Epoch [2/5], Loss: 0.1776\n",
      "Epoch [2/5], Loss: 0.1806\n",
      "Epoch [2/5], Loss: 0.1832\n",
      "Epoch [2/5], Loss: 0.1856\n",
      "Epoch [2/5], Loss: 0.1881\n",
      "Epoch [2/5], Loss: 0.1908\n",
      "Epoch [2/5], Loss: 0.1934\n",
      "Epoch [2/5], Loss: 0.1961\n",
      "Epoch [2/5], Loss: 0.1986\n",
      "Epoch [2/5], Loss: 0.2009\n",
      "Epoch [2/5], Loss: 0.2038\n",
      "Epoch [2/5], Loss: 0.2071\n",
      "Epoch [2/5], Loss: 0.2097\n",
      "Epoch [2/5], Loss: 0.2121\n",
      "Epoch [2/5], Loss: 0.2149\n",
      "Epoch [2/5], Loss: 0.2184\n",
      "Epoch [2/5], Loss: 0.2209\n",
      "Epoch [2/5], Loss: 0.2258\n",
      "Epoch [2/5], Loss: 0.2287\n",
      "Epoch [2/5], Loss: 0.2316\n",
      "Epoch [2/5], Loss: 0.2352\n",
      "Epoch [2/5], Loss: 0.2378\n",
      "Epoch [2/5], Loss: 0.2407\n",
      "Epoch [2/5], Loss: 0.2440\n",
      "Epoch [2/5], Loss: 0.2469\n",
      "Epoch [2/5], Loss: 0.2497\n",
      "Epoch [2/5], Loss: 0.2526\n",
      "Epoch [2/5], Loss: 0.2557\n",
      "Epoch [2/5], Loss: 0.2584\n",
      "Epoch [2/5], Loss: 0.2608\n",
      "Epoch [2/5], Loss: 0.2635\n",
      "Epoch [2/5], Loss: 0.2661\n",
      "Epoch [2/5], Loss: 0.2686\n",
      "Epoch [2/5], Loss: 0.2710\n",
      "Epoch [2/5], Loss: 0.2736\n",
      "Epoch [2/5], Loss: 0.2765\n",
      "Epoch [2/5], Loss: 0.2792\n",
      "Epoch [2/5], Loss: 0.2815\n",
      "Epoch [2/5], Loss: 0.2846\n",
      "Epoch [2/5], Loss: 0.2878\n",
      "Epoch [2/5], Loss: 0.2903\n",
      "Epoch [2/5], Loss: 0.2942\n",
      "Epoch [2/5], Loss: 0.2975\n",
      "Epoch [2/5], Loss: 0.2999\n",
      "Epoch [2/5], Loss: 0.3024\n",
      "Epoch [2/5], Loss: 0.3060\n",
      "Epoch [2/5], Loss: 0.3085\n",
      "Epoch [2/5], Loss: 0.3110\n",
      "Epoch [2/5], Loss: 0.3138\n",
      "Epoch [2/5], Loss: 0.3161\n",
      "Epoch [2/5], Loss: 0.3191\n",
      "Epoch [2/5], Loss: 0.3218\n",
      "Epoch [2/5], Loss: 0.3241\n",
      "Epoch [2/5], Loss: 0.3265\n",
      "Epoch [2/5], Loss: 0.3294\n",
      "Epoch [2/5], Loss: 0.3323\n",
      "Epoch [2/5], Loss: 0.3351\n",
      "Epoch [2/5], Loss: 0.3377\n",
      "Epoch [2/5], Loss: 0.3404\n",
      "Epoch [2/5], Loss: 0.3436\n",
      "Epoch [2/5], Loss: 0.3456\n",
      "Epoch [2/5], Loss: 0.3474\n",
      "Epoch [2/5], Loss: 0.3497\n",
      "Epoch [2/5], Loss: 0.3523\n",
      "Epoch [2/5], Loss: 0.3552\n",
      "Epoch [2/5], Loss: 0.3582\n",
      "Epoch [2/5], Loss: 0.3612\n",
      "Epoch [2/5], Loss: 0.3636\n",
      "Epoch [2/5], Loss: 0.3661\n",
      "Epoch [2/5], Loss: 0.3689\n",
      "Epoch [2/5], Loss: 0.3714\n",
      "Epoch [2/5], Loss: 0.3736\n",
      "Epoch [2/5], Loss: 0.3757\n",
      "Epoch [2/5], Loss: 0.3781\n",
      "Epoch [2/5], Loss: 0.3806\n",
      "Epoch [2/5], Loss: 0.3831\n",
      "Epoch [2/5], Loss: 0.3853\n",
      "Epoch [2/5], Loss: 0.3879\n",
      "Epoch [2/5], Loss: 0.3906\n",
      "Epoch [2/5], Loss: 0.3932\n",
      "Epoch [2/5], Loss: 0.3959\n",
      "Epoch [2/5], Loss: 0.3980\n",
      "Epoch [2/5], Loss: 0.4005\n",
      "Epoch [2/5], Loss: 0.4028\n",
      "Epoch [2/5], Loss: 0.4054\n",
      "Epoch [2/5], Loss: 0.4073\n",
      "Epoch [2/5], Loss: 0.4095\n",
      "Epoch [2/5], Loss: 0.4124\n",
      "Epoch [2/5], Loss: 0.4147\n",
      "Epoch [2/5], Loss: 0.4168\n",
      "Epoch [2/5], Loss: 0.4191\n",
      "Epoch [2/5], Loss: 0.4212\n",
      "Epoch [2/5], Loss: 0.4233\n",
      "Epoch [2/5], Loss: 0.4262\n",
      "Epoch [2/5], Loss: 0.4290\n",
      "Epoch [2/5], Loss: 0.4315\n",
      "Epoch [2/5], Loss: 0.4339\n",
      "Epoch [2/5], Loss: 0.4358\n",
      "Epoch [2/5], Loss: 0.4380\n",
      "Epoch [2/5], Loss: 0.4403\n",
      "Epoch [2/5], Loss: 0.4426\n",
      "Epoch [2/5], Loss: 0.4447\n",
      "Epoch [2/5], Loss: 0.4470\n",
      "Epoch [2/5], Loss: 0.4494\n",
      "Epoch [2/5], Loss: 0.4522\n",
      "Epoch [2/5], Loss: 0.4543\n",
      "Epoch [3/5], Loss: 0.0029\n",
      "Epoch [3/5], Loss: 0.0054\n",
      "Epoch [3/5], Loss: 0.0078\n",
      "Epoch [3/5], Loss: 0.0104\n",
      "Epoch [3/5], Loss: 0.0128\n",
      "Epoch [3/5], Loss: 0.0150\n",
      "Epoch [3/5], Loss: 0.0180\n",
      "Epoch [3/5], Loss: 0.0207\n",
      "Epoch [3/5], Loss: 0.0226\n",
      "Epoch [3/5], Loss: 0.0249\n",
      "Epoch [3/5], Loss: 0.0270\n",
      "Epoch [3/5], Loss: 0.0296\n",
      "Epoch [3/5], Loss: 0.0316\n",
      "Epoch [3/5], Loss: 0.0338\n",
      "Epoch [3/5], Loss: 0.0368\n",
      "Epoch [3/5], Loss: 0.0391\n",
      "Epoch [3/5], Loss: 0.0419\n",
      "Epoch [3/5], Loss: 0.0440\n",
      "Epoch [3/5], Loss: 0.0470\n",
      "Epoch [3/5], Loss: 0.0505\n",
      "Epoch [3/5], Loss: 0.0530\n",
      "Epoch [3/5], Loss: 0.0553\n",
      "Epoch [3/5], Loss: 0.0579\n",
      "Epoch [3/5], Loss: 0.0608\n",
      "Epoch [3/5], Loss: 0.0628\n",
      "Epoch [3/5], Loss: 0.0655\n",
      "Epoch [3/5], Loss: 0.0677\n",
      "Epoch [3/5], Loss: 0.0702\n",
      "Epoch [3/5], Loss: 0.0723\n",
      "Epoch [3/5], Loss: 0.0748\n",
      "Epoch [3/5], Loss: 0.0771\n",
      "Epoch [3/5], Loss: 0.0788\n",
      "Epoch [3/5], Loss: 0.0811\n",
      "Epoch [3/5], Loss: 0.0837\n",
      "Epoch [3/5], Loss: 0.0857\n",
      "Epoch [3/5], Loss: 0.0882\n",
      "Epoch [3/5], Loss: 0.0896\n",
      "Epoch [3/5], Loss: 0.0922\n",
      "Epoch [3/5], Loss: 0.0948\n",
      "Epoch [3/5], Loss: 0.0969\n",
      "Epoch [3/5], Loss: 0.0996\n",
      "Epoch [3/5], Loss: 0.1020\n",
      "Epoch [3/5], Loss: 0.1038\n",
      "Epoch [3/5], Loss: 0.1071\n",
      "Epoch [3/5], Loss: 0.1101\n",
      "Epoch [3/5], Loss: 0.1124\n",
      "Epoch [3/5], Loss: 0.1146\n",
      "Epoch [3/5], Loss: 0.1168\n",
      "Epoch [3/5], Loss: 0.1187\n",
      "Epoch [3/5], Loss: 0.1223\n",
      "Epoch [3/5], Loss: 0.1249\n",
      "Epoch [3/5], Loss: 0.1271\n",
      "Epoch [3/5], Loss: 0.1292\n",
      "Epoch [3/5], Loss: 0.1320\n",
      "Epoch [3/5], Loss: 0.1338\n",
      "Epoch [3/5], Loss: 0.1359\n",
      "Epoch [3/5], Loss: 0.1379\n",
      "Epoch [3/5], Loss: 0.1401\n",
      "Epoch [3/5], Loss: 0.1430\n",
      "Epoch [3/5], Loss: 0.1455\n",
      "Epoch [3/5], Loss: 0.1483\n",
      "Epoch [3/5], Loss: 0.1510\n",
      "Epoch [3/5], Loss: 0.1532\n",
      "Epoch [3/5], Loss: 0.1556\n",
      "Epoch [3/5], Loss: 0.1579\n",
      "Epoch [3/5], Loss: 0.1599\n",
      "Epoch [3/5], Loss: 0.1623\n",
      "Epoch [3/5], Loss: 0.1648\n",
      "Epoch [3/5], Loss: 0.1664\n",
      "Epoch [3/5], Loss: 0.1691\n",
      "Epoch [3/5], Loss: 0.1710\n",
      "Epoch [3/5], Loss: 0.1742\n",
      "Epoch [3/5], Loss: 0.1760\n",
      "Epoch [3/5], Loss: 0.1790\n",
      "Epoch [3/5], Loss: 0.1821\n",
      "Epoch [3/5], Loss: 0.1850\n",
      "Epoch [3/5], Loss: 0.1874\n",
      "Epoch [3/5], Loss: 0.1898\n",
      "Epoch [3/5], Loss: 0.1921\n",
      "Epoch [3/5], Loss: 0.1954\n",
      "Epoch [3/5], Loss: 0.1979\n",
      "Epoch [3/5], Loss: 0.2002\n",
      "Epoch [3/5], Loss: 0.2032\n",
      "Epoch [3/5], Loss: 0.2065\n",
      "Epoch [3/5], Loss: 0.2097\n",
      "Epoch [3/5], Loss: 0.2129\n",
      "Epoch [3/5], Loss: 0.2157\n",
      "Epoch [3/5], Loss: 0.2200\n",
      "Epoch [3/5], Loss: 0.2231\n",
      "Epoch [3/5], Loss: 0.2261\n",
      "Epoch [3/5], Loss: 0.2301\n",
      "Epoch [3/5], Loss: 0.2327\n",
      "Epoch [3/5], Loss: 0.2355\n",
      "Epoch [3/5], Loss: 0.2382\n",
      "Epoch [3/5], Loss: 0.2416\n",
      "Epoch [3/5], Loss: 0.2447\n",
      "Epoch [3/5], Loss: 0.2473\n",
      "Epoch [3/5], Loss: 0.2498\n",
      "Epoch [3/5], Loss: 0.2525\n",
      "Epoch [3/5], Loss: 0.2547\n",
      "Epoch [3/5], Loss: 0.2579\n",
      "Epoch [3/5], Loss: 0.2609\n",
      "Epoch [3/5], Loss: 0.2640\n",
      "Epoch [3/5], Loss: 0.2666\n",
      "Epoch [3/5], Loss: 0.2687\n",
      "Epoch [3/5], Loss: 0.2720\n",
      "Epoch [3/5], Loss: 0.2741\n",
      "Epoch [3/5], Loss: 0.2768\n",
      "Epoch [3/5], Loss: 0.2787\n",
      "Epoch [3/5], Loss: 0.2812\n",
      "Epoch [3/5], Loss: 0.2836\n",
      "Epoch [3/5], Loss: 0.2861\n",
      "Epoch [3/5], Loss: 0.2899\n",
      "Epoch [3/5], Loss: 0.2929\n",
      "Epoch [3/5], Loss: 0.2952\n",
      "Epoch [3/5], Loss: 0.2983\n",
      "Epoch [3/5], Loss: 0.3009\n",
      "Epoch [3/5], Loss: 0.3033\n",
      "Epoch [3/5], Loss: 0.3060\n",
      "Epoch [3/5], Loss: 0.3079\n",
      "Epoch [3/5], Loss: 0.3101\n",
      "Epoch [3/5], Loss: 0.3128\n",
      "Epoch [3/5], Loss: 0.3148\n",
      "Epoch [3/5], Loss: 0.3173\n",
      "Epoch [3/5], Loss: 0.3196\n",
      "Epoch [3/5], Loss: 0.3219\n",
      "Epoch [3/5], Loss: 0.3254\n",
      "Epoch [3/5], Loss: 0.3274\n",
      "Epoch [3/5], Loss: 0.3300\n",
      "Epoch [3/5], Loss: 0.3328\n",
      "Epoch [3/5], Loss: 0.3361\n",
      "Epoch [3/5], Loss: 0.3390\n",
      "Epoch [3/5], Loss: 0.3411\n",
      "Epoch [3/5], Loss: 0.3438\n",
      "Epoch [3/5], Loss: 0.3464\n",
      "Epoch [3/5], Loss: 0.3492\n",
      "Epoch [3/5], Loss: 0.3516\n",
      "Epoch [3/5], Loss: 0.3544\n",
      "Epoch [3/5], Loss: 0.3565\n",
      "Epoch [3/5], Loss: 0.3586\n",
      "Epoch [3/5], Loss: 0.3605\n",
      "Epoch [3/5], Loss: 0.3627\n",
      "Epoch [3/5], Loss: 0.3648\n",
      "Epoch [3/5], Loss: 0.3671\n",
      "Epoch [3/5], Loss: 0.3686\n",
      "Epoch [3/5], Loss: 0.3712\n",
      "Epoch [3/5], Loss: 0.3731\n",
      "Epoch [3/5], Loss: 0.3756\n",
      "Epoch [3/5], Loss: 0.3778\n",
      "Epoch [3/5], Loss: 0.3812\n",
      "Epoch [3/5], Loss: 0.3833\n",
      "Epoch [3/5], Loss: 0.3865\n",
      "Epoch [3/5], Loss: 0.3894\n",
      "Epoch [3/5], Loss: 0.3915\n",
      "Epoch [3/5], Loss: 0.3953\n",
      "Epoch [3/5], Loss: 0.3974\n",
      "Epoch [3/5], Loss: 0.4019\n",
      "Epoch [3/5], Loss: 0.4042\n",
      "Epoch [3/5], Loss: 0.4061\n",
      "Epoch [3/5], Loss: 0.4092\n",
      "Epoch [3/5], Loss: 0.4120\n",
      "Epoch [3/5], Loss: 0.4148\n",
      "Epoch [3/5], Loss: 0.4178\n",
      "Epoch [3/5], Loss: 0.4202\n",
      "Epoch [3/5], Loss: 0.4227\n",
      "Epoch [3/5], Loss: 0.4251\n",
      "Epoch [3/5], Loss: 0.4285\n",
      "Epoch [4/5], Loss: 0.0029\n",
      "Epoch [4/5], Loss: 0.0054\n",
      "Epoch [4/5], Loss: 0.0082\n",
      "Epoch [4/5], Loss: 0.0102\n",
      "Epoch [4/5], Loss: 0.0126\n",
      "Epoch [4/5], Loss: 0.0152\n",
      "Epoch [4/5], Loss: 0.0179\n",
      "Epoch [4/5], Loss: 0.0208\n",
      "Epoch [4/5], Loss: 0.0233\n",
      "Epoch [4/5], Loss: 0.0260\n",
      "Epoch [4/5], Loss: 0.0287\n",
      "Epoch [4/5], Loss: 0.0308\n",
      "Epoch [4/5], Loss: 0.0333\n",
      "Epoch [4/5], Loss: 0.0349\n",
      "Epoch [4/5], Loss: 0.0367\n",
      "Epoch [4/5], Loss: 0.0395\n",
      "Epoch [4/5], Loss: 0.0415\n",
      "Epoch [4/5], Loss: 0.0436\n",
      "Epoch [4/5], Loss: 0.0452\n",
      "Epoch [4/5], Loss: 0.0473\n",
      "Epoch [4/5], Loss: 0.0495\n",
      "Epoch [4/5], Loss: 0.0518\n",
      "Epoch [4/5], Loss: 0.0537\n",
      "Epoch [4/5], Loss: 0.0551\n",
      "Epoch [4/5], Loss: 0.0570\n",
      "Epoch [4/5], Loss: 0.0591\n",
      "Epoch [4/5], Loss: 0.0618\n",
      "Epoch [4/5], Loss: 0.0639\n",
      "Epoch [4/5], Loss: 0.0669\n",
      "Epoch [4/5], Loss: 0.0695\n",
      "Epoch [4/5], Loss: 0.0719\n",
      "Epoch [4/5], Loss: 0.0742\n",
      "Epoch [4/5], Loss: 0.0757\n",
      "Epoch [4/5], Loss: 0.0778\n",
      "Epoch [4/5], Loss: 0.0799\n",
      "Epoch [4/5], Loss: 0.0822\n",
      "Epoch [4/5], Loss: 0.0845\n",
      "Epoch [4/5], Loss: 0.0865\n",
      "Epoch [4/5], Loss: 0.0887\n",
      "Epoch [4/5], Loss: 0.0907\n",
      "Epoch [4/5], Loss: 0.0932\n",
      "Epoch [4/5], Loss: 0.0954\n",
      "Epoch [4/5], Loss: 0.0976\n",
      "Epoch [4/5], Loss: 0.0992\n",
      "Epoch [4/5], Loss: 0.1012\n",
      "Epoch [4/5], Loss: 0.1031\n",
      "Epoch [4/5], Loss: 0.1054\n",
      "Epoch [4/5], Loss: 0.1073\n",
      "Epoch [4/5], Loss: 0.1095\n",
      "Epoch [4/5], Loss: 0.1116\n",
      "Epoch [4/5], Loss: 0.1138\n",
      "Epoch [4/5], Loss: 0.1154\n",
      "Epoch [4/5], Loss: 0.1172\n",
      "Epoch [4/5], Loss: 0.1199\n",
      "Epoch [4/5], Loss: 0.1220\n",
      "Epoch [4/5], Loss: 0.1244\n",
      "Epoch [4/5], Loss: 0.1266\n",
      "Epoch [4/5], Loss: 0.1284\n",
      "Epoch [4/5], Loss: 0.1323\n",
      "Epoch [4/5], Loss: 0.1341\n",
      "Epoch [4/5], Loss: 0.1356\n",
      "Epoch [4/5], Loss: 0.1380\n",
      "Epoch [4/5], Loss: 0.1401\n",
      "Epoch [4/5], Loss: 0.1430\n",
      "Epoch [4/5], Loss: 0.1454\n",
      "Epoch [4/5], Loss: 0.1477\n",
      "Epoch [4/5], Loss: 0.1498\n",
      "Epoch [4/5], Loss: 0.1526\n",
      "Epoch [4/5], Loss: 0.1563\n",
      "Epoch [4/5], Loss: 0.1587\n",
      "Epoch [4/5], Loss: 0.1604\n",
      "Epoch [4/5], Loss: 0.1623\n",
      "Epoch [4/5], Loss: 0.1648\n",
      "Epoch [4/5], Loss: 0.1664\n",
      "Epoch [4/5], Loss: 0.1692\n",
      "Epoch [4/5], Loss: 0.1711\n",
      "Epoch [4/5], Loss: 0.1731\n",
      "Epoch [4/5], Loss: 0.1750\n",
      "Epoch [4/5], Loss: 0.1770\n",
      "Epoch [4/5], Loss: 0.1798\n",
      "Epoch [4/5], Loss: 0.1819\n",
      "Epoch [4/5], Loss: 0.1843\n",
      "Epoch [4/5], Loss: 0.1865\n",
      "Epoch [4/5], Loss: 0.1881\n",
      "Epoch [4/5], Loss: 0.1902\n",
      "Epoch [4/5], Loss: 0.1922\n",
      "Epoch [4/5], Loss: 0.1943\n",
      "Epoch [4/5], Loss: 0.1964\n",
      "Epoch [4/5], Loss: 0.1980\n",
      "Epoch [4/5], Loss: 0.1998\n",
      "Epoch [4/5], Loss: 0.2028\n",
      "Epoch [4/5], Loss: 0.2042\n",
      "Epoch [4/5], Loss: 0.2069\n",
      "Epoch [4/5], Loss: 0.2088\n",
      "Epoch [4/5], Loss: 0.2111\n",
      "Epoch [4/5], Loss: 0.2132\n",
      "Epoch [4/5], Loss: 0.2154\n",
      "Epoch [4/5], Loss: 0.2178\n",
      "Epoch [4/5], Loss: 0.2198\n",
      "Epoch [4/5], Loss: 0.2219\n",
      "Epoch [4/5], Loss: 0.2243\n",
      "Epoch [4/5], Loss: 0.2266\n",
      "Epoch [4/5], Loss: 0.2277\n",
      "Epoch [4/5], Loss: 0.2294\n",
      "Epoch [4/5], Loss: 0.2312\n",
      "Epoch [4/5], Loss: 0.2327\n",
      "Epoch [4/5], Loss: 0.2344\n",
      "Epoch [4/5], Loss: 0.2365\n",
      "Epoch [4/5], Loss: 0.2386\n",
      "Epoch [4/5], Loss: 0.2400\n",
      "Epoch [4/5], Loss: 0.2423\n",
      "Epoch [4/5], Loss: 0.2445\n",
      "Epoch [4/5], Loss: 0.2465\n",
      "Epoch [4/5], Loss: 0.2494\n",
      "Epoch [4/5], Loss: 0.2506\n",
      "Epoch [4/5], Loss: 0.2524\n",
      "Epoch [4/5], Loss: 0.2544\n",
      "Epoch [4/5], Loss: 0.2565\n",
      "Epoch [4/5], Loss: 0.2592\n",
      "Epoch [4/5], Loss: 0.2618\n",
      "Epoch [4/5], Loss: 0.2646\n",
      "Epoch [4/5], Loss: 0.2669\n",
      "Epoch [4/5], Loss: 0.2699\n",
      "Epoch [4/5], Loss: 0.2719\n",
      "Epoch [4/5], Loss: 0.2741\n",
      "Epoch [4/5], Loss: 0.2756\n",
      "Epoch [4/5], Loss: 0.2780\n",
      "Epoch [4/5], Loss: 0.2802\n",
      "Epoch [4/5], Loss: 0.2826\n",
      "Epoch [4/5], Loss: 0.2847\n",
      "Epoch [4/5], Loss: 0.2858\n",
      "Epoch [4/5], Loss: 0.2875\n",
      "Epoch [4/5], Loss: 0.2891\n",
      "Epoch [4/5], Loss: 0.2908\n",
      "Epoch [4/5], Loss: 0.2926\n",
      "Epoch [4/5], Loss: 0.2943\n",
      "Epoch [4/5], Loss: 0.2958\n",
      "Epoch [4/5], Loss: 0.2980\n",
      "Epoch [4/5], Loss: 0.2996\n",
      "Epoch [4/5], Loss: 0.3006\n",
      "Epoch [4/5], Loss: 0.3022\n",
      "Epoch [4/5], Loss: 0.3043\n",
      "Epoch [4/5], Loss: 0.3056\n",
      "Epoch [4/5], Loss: 0.3078\n",
      "Epoch [4/5], Loss: 0.3102\n",
      "Epoch [4/5], Loss: 0.3122\n",
      "Epoch [4/5], Loss: 0.3153\n",
      "Epoch [4/5], Loss: 0.3173\n",
      "Epoch [4/5], Loss: 0.3188\n",
      "Epoch [4/5], Loss: 0.3206\n",
      "Epoch [4/5], Loss: 0.3224\n",
      "Epoch [4/5], Loss: 0.3242\n",
      "Epoch [4/5], Loss: 0.3259\n",
      "Epoch [4/5], Loss: 0.3279\n",
      "Epoch [4/5], Loss: 0.3290\n",
      "Epoch [4/5], Loss: 0.3316\n",
      "Epoch [4/5], Loss: 0.3337\n",
      "Epoch [4/5], Loss: 0.3360\n",
      "Epoch [4/5], Loss: 0.3379\n",
      "Epoch [4/5], Loss: 0.3392\n",
      "Epoch [4/5], Loss: 0.3407\n",
      "Epoch [4/5], Loss: 0.3427\n",
      "Epoch [4/5], Loss: 0.3443\n",
      "Epoch [4/5], Loss: 0.3466\n",
      "Epoch [4/5], Loss: 0.3493\n",
      "Epoch [4/5], Loss: 0.3518\n",
      "Epoch [4/5], Loss: 0.3540\n",
      "Epoch [5/5], Loss: 0.0013\n",
      "Epoch [5/5], Loss: 0.0031\n",
      "Epoch [5/5], Loss: 0.0047\n",
      "Epoch [5/5], Loss: 0.0066\n",
      "Epoch [5/5], Loss: 0.0082\n",
      "Epoch [5/5], Loss: 0.0098\n",
      "Epoch [5/5], Loss: 0.0117\n",
      "Epoch [5/5], Loss: 0.0135\n",
      "Epoch [5/5], Loss: 0.0151\n",
      "Epoch [5/5], Loss: 0.0175\n",
      "Epoch [5/5], Loss: 0.0194\n",
      "Epoch [5/5], Loss: 0.0209\n",
      "Epoch [5/5], Loss: 0.0227\n",
      "Epoch [5/5], Loss: 0.0246\n",
      "Epoch [5/5], Loss: 0.0259\n",
      "Epoch [5/5], Loss: 0.0273\n",
      "Epoch [5/5], Loss: 0.0293\n",
      "Epoch [5/5], Loss: 0.0306\n",
      "Epoch [5/5], Loss: 0.0325\n",
      "Epoch [5/5], Loss: 0.0344\n",
      "Epoch [5/5], Loss: 0.0355\n",
      "Epoch [5/5], Loss: 0.0373\n",
      "Epoch [5/5], Loss: 0.0393\n",
      "Epoch [5/5], Loss: 0.0411\n",
      "Epoch [5/5], Loss: 0.0435\n",
      "Epoch [5/5], Loss: 0.0452\n",
      "Epoch [5/5], Loss: 0.0471\n",
      "Epoch [5/5], Loss: 0.0486\n",
      "Epoch [5/5], Loss: 0.0533\n",
      "Epoch [5/5], Loss: 0.0550\n",
      "Epoch [5/5], Loss: 0.0568\n",
      "Epoch [5/5], Loss: 0.0588\n",
      "Epoch [5/5], Loss: 0.0612\n",
      "Epoch [5/5], Loss: 0.0639\n",
      "Epoch [5/5], Loss: 0.0656\n",
      "Epoch [5/5], Loss: 0.0672\n",
      "Epoch [5/5], Loss: 0.0688\n",
      "Epoch [5/5], Loss: 0.0711\n",
      "Epoch [5/5], Loss: 0.0728\n",
      "Epoch [5/5], Loss: 0.0752\n",
      "Epoch [5/5], Loss: 0.0779\n",
      "Epoch [5/5], Loss: 0.0801\n",
      "Epoch [5/5], Loss: 0.0831\n",
      "Epoch [5/5], Loss: 0.0847\n",
      "Epoch [5/5], Loss: 0.0872\n",
      "Epoch [5/5], Loss: 0.0893\n",
      "Epoch [5/5], Loss: 0.0909\n",
      "Epoch [5/5], Loss: 0.0929\n",
      "Epoch [5/5], Loss: 0.0947\n",
      "Epoch [5/5], Loss: 0.0968\n",
      "Epoch [5/5], Loss: 0.0991\n",
      "Epoch [5/5], Loss: 0.1014\n",
      "Epoch [5/5], Loss: 0.1037\n",
      "Epoch [5/5], Loss: 0.1065\n",
      "Epoch [5/5], Loss: 0.1086\n",
      "Epoch [5/5], Loss: 0.1112\n",
      "Epoch [5/5], Loss: 0.1135\n",
      "Epoch [5/5], Loss: 0.1162\n",
      "Epoch [5/5], Loss: 0.1188\n",
      "Epoch [5/5], Loss: 0.1205\n",
      "Epoch [5/5], Loss: 0.1220\n",
      "Epoch [5/5], Loss: 0.1240\n",
      "Epoch [5/5], Loss: 0.1256\n",
      "Epoch [5/5], Loss: 0.1270\n",
      "Epoch [5/5], Loss: 0.1287\n",
      "Epoch [5/5], Loss: 0.1314\n",
      "Epoch [5/5], Loss: 0.1336\n",
      "Epoch [5/5], Loss: 0.1353\n",
      "Epoch [5/5], Loss: 0.1378\n",
      "Epoch [5/5], Loss: 0.1393\n",
      "Epoch [5/5], Loss: 0.1410\n",
      "Epoch [5/5], Loss: 0.1429\n",
      "Epoch [5/5], Loss: 0.1451\n",
      "Epoch [5/5], Loss: 0.1471\n",
      "Epoch [5/5], Loss: 0.1492\n",
      "Epoch [5/5], Loss: 0.1506\n",
      "Epoch [5/5], Loss: 0.1526\n",
      "Epoch [5/5], Loss: 0.1544\n",
      "Epoch [5/5], Loss: 0.1558\n",
      "Epoch [5/5], Loss: 0.1580\n",
      "Epoch [5/5], Loss: 0.1596\n",
      "Epoch [5/5], Loss: 0.1614\n",
      "Epoch [5/5], Loss: 0.1628\n",
      "Epoch [5/5], Loss: 0.1649\n",
      "Epoch [5/5], Loss: 0.1673\n",
      "Epoch [5/5], Loss: 0.1697\n",
      "Epoch [5/5], Loss: 0.1723\n",
      "Epoch [5/5], Loss: 0.1738\n",
      "Epoch [5/5], Loss: 0.1752\n",
      "Epoch [5/5], Loss: 0.1765\n",
      "Epoch [5/5], Loss: 0.1785\n",
      "Epoch [5/5], Loss: 0.1809\n",
      "Epoch [5/5], Loss: 0.1821\n",
      "Epoch [5/5], Loss: 0.1849\n",
      "Epoch [5/5], Loss: 0.1861\n",
      "Epoch [5/5], Loss: 0.1882\n",
      "Epoch [5/5], Loss: 0.1896\n",
      "Epoch [5/5], Loss: 0.1911\n",
      "Epoch [5/5], Loss: 0.1925\n",
      "Epoch [5/5], Loss: 0.1945\n",
      "Epoch [5/5], Loss: 0.1966\n",
      "Epoch [5/5], Loss: 0.1987\n",
      "Epoch [5/5], Loss: 0.2006\n",
      "Epoch [5/5], Loss: 0.2029\n",
      "Epoch [5/5], Loss: 0.2040\n",
      "Epoch [5/5], Loss: 0.2061\n",
      "Epoch [5/5], Loss: 0.2078\n",
      "Epoch [5/5], Loss: 0.2091\n",
      "Epoch [5/5], Loss: 0.2118\n",
      "Epoch [5/5], Loss: 0.2134\n",
      "Epoch [5/5], Loss: 0.2154\n",
      "Epoch [5/5], Loss: 0.2183\n",
      "Epoch [5/5], Loss: 0.2200\n",
      "Epoch [5/5], Loss: 0.2225\n",
      "Epoch [5/5], Loss: 0.2245\n",
      "Epoch [5/5], Loss: 0.2262\n",
      "Epoch [5/5], Loss: 0.2288\n",
      "Epoch [5/5], Loss: 0.2308\n",
      "Epoch [5/5], Loss: 0.2323\n",
      "Epoch [5/5], Loss: 0.2338\n",
      "Epoch [5/5], Loss: 0.2355\n",
      "Epoch [5/5], Loss: 0.2371\n",
      "Epoch [5/5], Loss: 0.2391\n",
      "Epoch [5/5], Loss: 0.2415\n",
      "Epoch [5/5], Loss: 0.2440\n",
      "Epoch [5/5], Loss: 0.2460\n",
      "Epoch [5/5], Loss: 0.2481\n",
      "Epoch [5/5], Loss: 0.2507\n",
      "Epoch [5/5], Loss: 0.2521\n",
      "Epoch [5/5], Loss: 0.2541\n",
      "Epoch [5/5], Loss: 0.2558\n",
      "Epoch [5/5], Loss: 0.2582\n",
      "Epoch [5/5], Loss: 0.2596\n",
      "Epoch [5/5], Loss: 0.2612\n",
      "Epoch [5/5], Loss: 0.2627\n",
      "Epoch [5/5], Loss: 0.2645\n",
      "Epoch [5/5], Loss: 0.2661\n",
      "Epoch [5/5], Loss: 0.2688\n",
      "Epoch [5/5], Loss: 0.2715\n",
      "Epoch [5/5], Loss: 0.2732\n",
      "Epoch [5/5], Loss: 0.2759\n",
      "Epoch [5/5], Loss: 0.2776\n",
      "Epoch [5/5], Loss: 0.2797\n",
      "Epoch [5/5], Loss: 0.2818\n",
      "Epoch [5/5], Loss: 0.2833\n",
      "Epoch [5/5], Loss: 0.2852\n",
      "Epoch [5/5], Loss: 0.2879\n",
      "Epoch [5/5], Loss: 0.2896\n",
      "Epoch [5/5], Loss: 0.2909\n",
      "Epoch [5/5], Loss: 0.2928\n",
      "Epoch [5/5], Loss: 0.2940\n",
      "Epoch [5/5], Loss: 0.2954\n",
      "Epoch [5/5], Loss: 0.2975\n",
      "Epoch [5/5], Loss: 0.3003\n",
      "Epoch [5/5], Loss: 0.3026\n",
      "Epoch [5/5], Loss: 0.3052\n",
      "Epoch [5/5], Loss: 0.3069\n",
      "Epoch [5/5], Loss: 0.3083\n",
      "Epoch [5/5], Loss: 0.3100\n",
      "Epoch [5/5], Loss: 0.3117\n",
      "Epoch [5/5], Loss: 0.3131\n",
      "Epoch [5/5], Loss: 0.3142\n",
      "Epoch [5/5], Loss: 0.3167\n",
      "Epoch [5/5], Loss: 0.3195\n",
      "Epoch [5/5], Loss: 0.3213\n",
      "Epoch [5/5], Loss: 0.3232\n",
      "Epoch [5/5], Loss: 0.3258\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 95.31%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 78.12%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 81.25%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 81.25%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 79.69%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 78.12%\n",
      "Accuracy of the network on the 64 test images: 90.62%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 79.69%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 92.19%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 90.62%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 90.62%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 76.56%\n",
      "Accuracy of the network on the 64 test images: 81.25%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 89.06%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 81.25%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 85.94%\n",
      "Accuracy of the network on the 64 test images: 84.38%\n",
      "Accuracy of the network on the 64 test images: 79.69%\n",
      "Accuracy of the network on the 64 test images: 81.25%\n",
      "Accuracy of the network on the 64 test images: 87.50%\n",
      "Accuracy of the network on the 64 test images: 82.81%\n",
      "Accuracy of the network on the 41 test images: 87.80%\n",
      "TotalTotal  Accuracy of the network on the 41 test images: 87.80%\n"
     ]
    }
   ],
   "source": [
    "#Funcion de entrenamiento\n",
    "def train_model(model,train_loader,criterion,optimizer,num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs,labels = inputs.to(device),labels.to(device)\n",
    "\n",
    "            #Paso forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            #Paso backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "train_model(model,train_loader,criterion,optimizer)\n",
    "\n",
    "#Funcion de eval\n",
    "def evaluate_model(model,test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct_sum = 0\n",
    "    total = 0\n",
    "    total_sum = 0\n",
    "    for inputs,labels in test_loader:\n",
    "            inputs,labels = inputs.to(device),labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total =+ labels.size(0)\n",
    "            total_sum =+ total\n",
    "            correct =+ (predicted == labels).sum().item()\n",
    "            correct_sum =+ correct\n",
    "            print(f'Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "    print(f'TotalTotal  Accuracy of the network on the {total_sum} test images: {100 * correct_sum / total_sum:.2f}%')\n",
    "evaluate_model(model,test_loader)\n",
    "torch.save(model,'skincancer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099338-b56a-41f1-b240-09b541f732dc",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f31867-6f7d-4f7f-84fa-41aef0990819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5534/693354255.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"skincancer.pth\")\n"
     ]
    }
   ],
   "source": [
    "def saveTrainedModel():\n",
    "    torch.save(model,'skincancer.pth')\n",
    "\n",
    "def chargeTrainedModel():\n",
    "    model = MelanomaDetector()\n",
    "    model.load_state_dict(torch.load('skincancer.pth'))\n",
    "    #model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = torch.load(\"skincancer.pth\")\n",
    "#print(model)\n",
    "\n",
    "saveTrainedModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4b1d5-2d37-4622-bcc0-a10fc9cb6513",
   "metadata": {},
   "source": [
    "## Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4025ccc1-761f-44d9-a21d-c7ac9f034de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms, io\n",
    "def individual(path):\n",
    "    model = chargeTrainedModel().to(device)\n",
    "    image_input = io.read_image(path).to(device)\n",
    "\n",
    "    image_input = image_input.reshape(1,3,224,224)/255\n",
    "    \n",
    "    print(image_input.shape)\n",
    "    logits = model(image_input) # [1, 3, 224, 224]\n",
    "    \n",
    "    print(logits)\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    print(probabilities)\n",
    "    if torch.argmax(probabilities) == 0:\n",
    "        result = \"cancer\"\n",
    "    else:\n",
    "        result = \"no cancer\"\n",
    "    print(\"Enhorabona, el resultat és: \", result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0916834a-0731-4a87-b134-9c6f91a058b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5534/693354255.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('skincancer.pth'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class '__main__.MelanomaDetector'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindividual\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./prueba/AUG_0_1.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mindividual\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindividual\u001b[39m(path):\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mchargeTrainedModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m     image_input \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mread_image(path)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     image_input \u001b[38;5;241m=\u001b[39m image_input\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mchargeTrainedModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchargeTrainedModel\u001b[39m():\n\u001b[1;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m MelanomaDetector()\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskincancer.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documentos/IA/PyTorch/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2516\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \n\u001b[1;32m   2481\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2517\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2518\u001b[0m     )\n\u001b[1;32m   2520\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2521\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class '__main__.MelanomaDetector'>."
     ]
    }
   ],
   "source": [
    "individual(\"./prueba/AUG_0_1.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f21a9-777c-4a58-9a7b-a02072c0631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[10,0,0,0]], dtype=torch.float32)\n",
    "print(torch.softmax(tensor, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b42e10-f021-4e87-8a74-6bfae56ba91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
