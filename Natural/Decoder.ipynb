{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4eacd9c-b250-47e6-83da-4bf9ec01d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iticbcn/Documentos/IA/Natural/natural/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ca785d-67c2-48c9-82e9-dc9a94a2e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b94fb6-d64e-457a-a07c-20c2102bc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "with open(\"el_quijote.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    contenido = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c027217-e45f-4026-90f1-35bf07e539bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(contenido):\n",
    "    tokens = tokenizer.encode(contenido,max_length=512,truncation=True)\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652ad8d3-f67d-4961-a9dd-8730bf11e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "class text_dataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, input_length=10, context_size=50):\n",
    "        tokens = tokenizer.encode(text)\n",
    "        self.sentences = [tokens[i*input_length:i*input_length+input_length] for i in range(len(tokens)//10)]\n",
    "        \n",
    "        self.sentences_per_unit = input_length-1\n",
    "        self.context_size = context_size\n",
    "                     \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences*self.sentences_per_unit)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx//self.sentences_per_unit]\n",
    "        #print(idx%self.sentences_per_unit+1)\n",
    "        x = sentence[0:idx%self.sentences_per_unit+1]\n",
    "        y = sentence[idx%self.sentences_per_unit+1]\n",
    "\n",
    "        if len(x) < self.context_size:\n",
    "            for i in range(len(x),self.context_size):\n",
    "                x.append(0)\n",
    "        \n",
    "        return torch.tensor(x), y\n",
    "\n",
    "dataset = text_dataset(contenido, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f17eca4-f5d0-4834-b98b-2e0e47b9db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "3678\n"
     ]
    }
   ],
   "source": [
    "x, y = dataset[109]\n",
    "print(len(x))\n",
    "print(y)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, shuffle=True, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a5819e-6957-48d3-8e20-41f966cbd1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets(dataset, split_ratio=0.8):\n",
    "    train_size = int(split_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    return train_data, test_data\n",
    "def create_dataloader(training_data, test_data, batch_size):\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5de52e-37f6-440b-b701-f247f3a23202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device():\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(device)\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1bdbf34-cccb-42e9-a1be-f3a271e0221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iticbcn/Documentos/IA/Natural/natural/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (embedding): Embedding(30522, 100)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (flatt): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      "  (fc): Linear(in_features=5000, out_features=30522, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=100, encoders=4, head=10):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Embedding layer para convertir palabras en vectores densos\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Capa Transformer Encoder\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=head, batch_first=True)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=encoders)\n",
    "\n",
    "        # Decoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "\n",
    "        # input = [batch_size, context_length, d_model]\n",
    "        \n",
    "        # Capa de regularización\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.flatt = nn.Flatten()\n",
    "\n",
    "        # Activación ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Capa de salida (embed_size → num_classes)\n",
    "        self.fc = nn.Linear(embed_size*50, vocab_size)  # Cambié hidden_size a embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convierte tokens en vectores de tamaño embed_size\n",
    "        x = self.transformer_encoder(x)  # Pasa por el Transformer Encoder\n",
    "        #x = torch.mean(x, dim=1)  # Promediamos los embeddings de la secuencia para clasificación\n",
    "        x = self.dropout(self.flatt(x))  # Aplicamos dropout\n",
    "        x = self.relu(x)  # Aplicamos ReLU\n",
    "        logits = self.fc(x)  # Capa de clasificación final\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(tokenizer.vocab_size)\n",
    "print(model.to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40051982-a031-4cc6-adcb-87591a8a8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 25 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    global best_weights, best_correct\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct/y.size(0)):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    if best_correct < correct:\n",
    "        best_correct = correct\n",
    "        best_weights = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55980255-3c5d-4e88-80b4-69a1599cfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_dataloader, loss_fn, optimizer, device, epochs, learning_rate):\n",
    "    global best_correct, best_weights\n",
    "    next_train = True\n",
    "    \n",
    "    while next_train:\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "            #best_correct, best_weights = test_loop(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "        print(\"Done! Saving model...\")\n",
    "        torch.save(model.state_dict(), 'encoder.pth')\n",
    "        print(f\"Best Accuracy: {(100 * best_correct):>0.1f}% Using Learning Rate: {learning_rate}\")\n",
    "\n",
    "        # Preguntar si continuar entrenando\n",
    "        stop_train = input(\"Should we continue training? No(Any) Yes(Enter) -> \")\n",
    "        if stop_train:\n",
    "            next_train = False\n",
    "        else:\n",
    "            # Opción para cambiar el learning rate\n",
    "            change_lr = input(\"Change the learning rate? Yes(Any) No(Enter) -> \")\n",
    "            if change_lr:\n",
    "                try:\n",
    "                    learning_rate = float(input(\"New learning rate: \"))\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = learning_rate\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Keeping the old learning rate.\")\n",
    "\n",
    "            # Opción para cambiar el número de epochs\n",
    "            change_epochs = input(f\"Change the number of epochs ({epochs})? Yes(Any) No(Enter) -> \")\n",
    "            if change_epochs:\n",
    "                try:\n",
    "                    epochs = int(input(\"New number of epochs: \"))\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Keeping the old number of epochs.\")\n",
    "\n",
    "def main(learning_rate, batch_size, epochs):\n",
    "    global best_correct, best_weights\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    device = select_device()\n",
    "\n",
    "    # Cargar modelo BERT\n",
    "    model = NeuralNetwork(tokenizer.vocab_size)\n",
    "    model.to(device)\n",
    "\n",
    "    # Preguntar si entrenar\n",
    "    train_on = input(\"Do you want to train? Yes(y), No(enter): \").strip().lower\n",
    "    with open(\"el_quijote.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        contenido = file.read()\n",
    "    #data = tokenizar(contenido)\n",
    "    #training_data, test_data = create_train_test_datasets(data)\n",
    "    #train_dataloader, test_dataloader = create_dataloader(training_data, test_data, batch_size)\n",
    "    train_dataset = text_dataset(contenido, tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if train_on in [\"y\", \"yes\"]:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        train_model(model, train_dataloader, loss_fn, optimizer, device, epochs, learning_rate)\n",
    "    else:\n",
    "        exit()\n",
    "    #    best_correct, best_weights = test_loop(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    best_correct = 0\n",
    "    best_weights = None\n",
    "    learning_rate = 0.0008\n",
    "    batch_size = 10\n",
    "    epochs = 10\n",
    "    #main(learning_rate, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773c2d64-4146-4f7b-a430-80d23389113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (350115 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.475163  [   10/315099]\n",
      "loss: 7.797037  [  260/315099]\n",
      "loss: 8.731383  [  510/315099]\n",
      "loss: 7.691641  [  760/315099]\n",
      "loss: 7.376943  [ 1010/315099]\n",
      "loss: 8.634453  [ 1260/315099]\n",
      "loss: 6.517761  [ 1510/315099]\n",
      "loss: 5.717522  [ 1760/315099]\n",
      "loss: 6.865970  [ 2010/315099]\n",
      "loss: 6.063924  [ 2260/315099]\n",
      "loss: 7.225429  [ 2510/315099]\n",
      "loss: 6.756115  [ 2760/315099]\n",
      "loss: 7.339614  [ 3010/315099]\n",
      "loss: 6.445864  [ 3260/315099]\n",
      "loss: 6.926076  [ 3510/315099]\n",
      "loss: 6.398129  [ 3760/315099]\n",
      "loss: 7.035302  [ 4010/315099]\n",
      "loss: 7.657014  [ 4260/315099]\n",
      "loss: 6.077056  [ 4510/315099]\n",
      "loss: 8.208899  [ 4760/315099]\n",
      "loss: 5.791764  [ 5010/315099]\n",
      "loss: 4.782477  [ 5260/315099]\n",
      "loss: 6.575049  [ 5510/315099]\n",
      "loss: 5.769012  [ 5760/315099]\n",
      "loss: 6.557657  [ 6010/315099]\n",
      "loss: 6.823188  [ 6260/315099]\n",
      "loss: 6.579680  [ 6510/315099]\n",
      "loss: 6.421004  [ 6760/315099]\n",
      "loss: 6.762350  [ 7010/315099]\n",
      "loss: 5.327163  [ 7260/315099]\n",
      "loss: 5.831066  [ 7510/315099]\n",
      "loss: 5.783262  [ 7760/315099]\n",
      "loss: 7.573931  [ 8010/315099]\n",
      "loss: 5.617992  [ 8260/315099]\n",
      "loss: 6.457614  [ 8510/315099]\n",
      "loss: 5.808860  [ 8760/315099]\n",
      "loss: 7.971766  [ 9010/315099]\n",
      "loss: 5.051017  [ 9260/315099]\n",
      "loss: 5.403193  [ 9510/315099]\n",
      "loss: 6.271926  [ 9760/315099]\n",
      "loss: 6.655220  [10010/315099]\n",
      "loss: 6.976804  [10260/315099]\n",
      "loss: 6.768848  [10510/315099]\n",
      "loss: 5.648818  [10760/315099]\n",
      "loss: 6.228003  [11010/315099]\n",
      "loss: 7.514190  [11260/315099]\n",
      "loss: 5.836662  [11510/315099]\n",
      "loss: 6.299199  [11760/315099]\n",
      "loss: 5.249055  [12010/315099]\n",
      "loss: 5.934493  [12260/315099]\n",
      "loss: 5.730049  [12510/315099]\n",
      "loss: 6.653695  [12760/315099]\n",
      "loss: 7.276994  [13010/315099]\n",
      "loss: 8.309422  [13260/315099]\n",
      "loss: 6.928389  [13510/315099]\n",
      "loss: 7.066620  [13760/315099]\n",
      "loss: 6.599127  [14010/315099]\n",
      "loss: 6.763340  [14260/315099]\n",
      "loss: 5.183400  [14510/315099]\n",
      "loss: 6.613360  [14760/315099]\n",
      "loss: 6.029335  [15010/315099]\n",
      "loss: 6.661315  [15260/315099]\n",
      "loss: 6.413087  [15510/315099]\n",
      "loss: 4.903525  [15760/315099]\n",
      "loss: 6.020678  [16010/315099]\n",
      "loss: 5.656955  [16260/315099]\n",
      "loss: 6.133077  [16510/315099]\n",
      "loss: 6.460135  [16760/315099]\n",
      "loss: 5.812141  [17010/315099]\n",
      "loss: 6.666548  [17260/315099]\n",
      "loss: 5.938434  [17510/315099]\n",
      "loss: 5.236691  [17760/315099]\n",
      "loss: 6.420697  [18010/315099]\n",
      "loss: 5.628888  [18260/315099]\n",
      "loss: 5.287876  [18510/315099]\n",
      "loss: 6.821597  [18760/315099]\n",
      "loss: 4.677279  [19010/315099]\n",
      "loss: 6.299932  [19260/315099]\n",
      "loss: 5.099368  [19510/315099]\n",
      "loss: 7.042842  [19760/315099]\n",
      "loss: 5.935795  [20010/315099]\n",
      "loss: 6.660102  [20260/315099]\n",
      "loss: 5.649763  [20510/315099]\n",
      "loss: 5.316857  [20760/315099]\n",
      "loss: 6.306735  [21010/315099]\n",
      "loss: 6.696383  [21260/315099]\n",
      "loss: 6.201104  [21510/315099]\n",
      "loss: 5.131399  [21760/315099]\n",
      "loss: 5.999392  [22010/315099]\n",
      "loss: 6.516181  [22260/315099]\n",
      "loss: 6.191048  [22510/315099]\n",
      "loss: 6.377901  [22760/315099]\n",
      "loss: 5.907374  [23010/315099]\n",
      "loss: 6.520990  [23260/315099]\n",
      "loss: 5.850852  [23510/315099]\n",
      "loss: 5.861150  [23760/315099]\n",
      "loss: 6.800325  [24010/315099]\n",
      "loss: 6.011761  [24260/315099]\n",
      "loss: 6.732143  [24510/315099]\n",
      "loss: 7.415998  [24760/315099]\n",
      "loss: 5.929482  [25010/315099]\n",
      "loss: 5.836443  [25260/315099]\n",
      "loss: 5.751578  [25510/315099]\n",
      "loss: 6.247325  [25760/315099]\n",
      "loss: 5.632337  [26010/315099]\n",
      "loss: 6.819983  [26260/315099]\n",
      "loss: 5.464374  [26510/315099]\n",
      "loss: 7.209607  [26760/315099]\n",
      "loss: 5.054534  [27010/315099]\n",
      "loss: 5.412658  [27260/315099]\n",
      "loss: 6.696165  [27510/315099]\n",
      "loss: 7.938467  [27760/315099]\n",
      "loss: 5.609616  [28010/315099]\n",
      "loss: 6.220294  [28260/315099]\n",
      "loss: 6.681316  [28510/315099]\n",
      "loss: 6.395193  [28760/315099]\n",
      "loss: 5.832315  [29010/315099]\n",
      "loss: 6.207092  [29260/315099]\n",
      "loss: 5.998240  [29510/315099]\n",
      "loss: 5.912873  [29760/315099]\n",
      "loss: 6.325085  [30010/315099]\n",
      "loss: 6.398558  [30260/315099]\n",
      "loss: 5.667741  [30510/315099]\n",
      "loss: 5.620293  [30760/315099]\n",
      "loss: 6.170287  [31010/315099]\n",
      "loss: 5.835028  [31260/315099]\n",
      "loss: 5.752615  [31510/315099]\n",
      "loss: 6.679194  [31760/315099]\n",
      "loss: 6.199764  [32010/315099]\n",
      "loss: 6.822980  [32260/315099]\n",
      "loss: 6.447314  [32510/315099]\n",
      "loss: 5.250037  [32760/315099]\n",
      "loss: 6.680522  [33010/315099]\n",
      "loss: 6.274198  [33260/315099]\n",
      "loss: 5.890971  [33510/315099]\n",
      "loss: 6.620358  [33760/315099]\n",
      "loss: 8.360284  [34010/315099]\n",
      "loss: 6.011266  [34260/315099]\n",
      "loss: 6.462639  [34510/315099]\n",
      "loss: 5.577457  [34760/315099]\n",
      "loss: 6.814655  [35010/315099]\n",
      "loss: 7.584027  [35260/315099]\n",
      "loss: 5.896012  [35510/315099]\n",
      "loss: 5.854835  [35760/315099]\n",
      "loss: 6.140153  [36010/315099]\n",
      "loss: 5.468884  [36260/315099]\n",
      "loss: 7.216327  [36510/315099]\n",
      "loss: 7.598353  [36760/315099]\n",
      "loss: 6.858983  [37010/315099]\n",
      "loss: 5.693404  [37260/315099]\n",
      "loss: 5.692300  [37510/315099]\n",
      "loss: 5.191919  [37760/315099]\n",
      "loss: 6.203061  [38010/315099]\n",
      "loss: 5.489314  [38260/315099]\n",
      "loss: 5.861886  [38510/315099]\n",
      "loss: 5.928172  [38760/315099]\n",
      "loss: 5.778114  [39010/315099]\n",
      "loss: 6.487079  [39260/315099]\n",
      "loss: 7.066604  [39510/315099]\n",
      "loss: 5.154119  [39760/315099]\n",
      "loss: 6.444463  [40010/315099]\n",
      "loss: 7.030781  [40260/315099]\n",
      "loss: 5.045849  [40510/315099]\n",
      "loss: 6.327843  [40760/315099]\n",
      "loss: 6.181169  [41010/315099]\n",
      "loss: 5.658866  [41260/315099]\n",
      "loss: 5.638323  [41510/315099]\n",
      "loss: 5.171059  [41760/315099]\n",
      "loss: 6.522584  [42010/315099]\n",
      "loss: 7.380032  [42260/315099]\n",
      "loss: 7.121626  [42510/315099]\n",
      "loss: 5.685727  [42760/315099]\n",
      "loss: 7.219386  [43010/315099]\n",
      "loss: 5.920749  [43260/315099]\n",
      "loss: 6.884030  [43510/315099]\n",
      "loss: 5.991061  [43760/315099]\n",
      "loss: 6.675957  [44010/315099]\n",
      "loss: 7.369115  [44260/315099]\n",
      "loss: 5.477331  [44510/315099]\n",
      "loss: 5.320326  [44760/315099]\n",
      "loss: 6.104658  [45010/315099]\n",
      "loss: 5.312709  [45260/315099]\n",
      "loss: 5.337245  [45510/315099]\n",
      "loss: 5.603543  [45760/315099]\n",
      "loss: 5.849940  [46010/315099]\n",
      "loss: 6.510411  [46260/315099]\n",
      "loss: 6.050874  [46510/315099]\n",
      "loss: 5.480645  [46760/315099]\n",
      "loss: 6.198275  [47010/315099]\n",
      "loss: 6.882272  [47260/315099]\n",
      "loss: 6.251027  [47510/315099]\n",
      "loss: 6.361470  [47760/315099]\n",
      "loss: 8.168305  [48010/315099]\n",
      "loss: 5.675730  [48260/315099]\n",
      "loss: 5.770631  [48510/315099]\n",
      "loss: 6.680386  [48760/315099]\n",
      "loss: 5.806758  [49010/315099]\n",
      "loss: 5.272769  [49260/315099]\n",
      "loss: 7.482551  [49510/315099]\n",
      "loss: 5.976755  [49760/315099]\n",
      "loss: 5.363586  [50010/315099]\n",
      "loss: 5.719031  [50260/315099]\n",
      "loss: 5.001954  [50510/315099]\n",
      "loss: 7.690009  [50760/315099]\n",
      "loss: 5.803739  [51010/315099]\n",
      "loss: 6.419321  [51260/315099]\n",
      "loss: 7.350713  [51510/315099]\n",
      "loss: 6.775296  [51760/315099]\n",
      "loss: 6.442661  [52010/315099]\n",
      "loss: 6.739338  [52260/315099]\n",
      "loss: 5.508424  [52510/315099]\n",
      "loss: 5.459075  [52760/315099]\n",
      "loss: 5.929794  [53010/315099]\n",
      "loss: 7.728540  [53260/315099]\n",
      "loss: 6.493001  [53510/315099]\n",
      "loss: 5.294175  [53760/315099]\n",
      "loss: 6.520219  [54010/315099]\n",
      "loss: 4.826159  [54260/315099]\n",
      "loss: 5.556980  [54510/315099]\n",
      "loss: 7.101362  [54760/315099]\n",
      "loss: 5.636081  [55010/315099]\n",
      "loss: 6.885488  [55260/315099]\n",
      "loss: 5.504478  [55510/315099]\n",
      "loss: 6.570573  [55760/315099]\n",
      "loss: 5.122829  [56010/315099]\n",
      "loss: 5.198850  [56260/315099]\n",
      "loss: 7.539015  [56510/315099]\n",
      "loss: 5.467700  [56760/315099]\n",
      "loss: 5.991294  [57010/315099]\n",
      "loss: 6.171938  [57260/315099]\n",
      "loss: 5.106388  [57510/315099]\n",
      "loss: 5.617459  [57760/315099]\n",
      "loss: 5.187066  [58010/315099]\n",
      "loss: 4.858091  [58260/315099]\n",
      "loss: 4.935361  [58510/315099]\n",
      "loss: 6.484380  [58760/315099]\n",
      "loss: 6.056832  [59010/315099]\n",
      "loss: 5.895131  [59260/315099]\n",
      "loss: 6.274700  [59510/315099]\n",
      "loss: 6.055471  [59760/315099]\n",
      "loss: 5.973835  [60010/315099]\n",
      "loss: 6.430270  [60260/315099]\n",
      "loss: 6.446624  [60510/315099]\n",
      "loss: 5.758777  [60760/315099]\n",
      "loss: 5.089220  [61010/315099]\n",
      "loss: 5.927609  [61260/315099]\n",
      "loss: 5.211810  [61510/315099]\n",
      "loss: 5.513970  [61760/315099]\n",
      "loss: 5.693422  [62010/315099]\n",
      "loss: 5.622043  [62260/315099]\n",
      "loss: 5.788537  [62510/315099]\n",
      "loss: 5.613299  [62760/315099]\n",
      "loss: 6.264131  [63010/315099]\n",
      "loss: 5.419546  [63260/315099]\n",
      "loss: 5.604558  [63510/315099]\n",
      "loss: 5.628812  [63760/315099]\n",
      "loss: 6.935403  [64010/315099]\n",
      "loss: 7.796718  [64260/315099]\n",
      "loss: 6.970999  [64510/315099]\n",
      "loss: 5.474109  [64760/315099]\n",
      "loss: 4.923465  [65010/315099]\n",
      "loss: 5.666405  [65260/315099]\n",
      "loss: 6.099903  [65510/315099]\n",
      "loss: 5.461733  [65760/315099]\n",
      "loss: 6.030417  [66010/315099]\n",
      "loss: 5.791337  [66260/315099]\n",
      "loss: 6.218122  [66510/315099]\n",
      "loss: 6.088621  [66760/315099]\n",
      "loss: 6.553426  [67010/315099]\n",
      "loss: 6.238891  [67260/315099]\n",
      "loss: 6.108868  [67510/315099]\n",
      "loss: 5.654950  [67760/315099]\n",
      "loss: 7.224724  [68010/315099]\n",
      "loss: 5.778281  [68260/315099]\n",
      "loss: 5.650553  [68510/315099]\n",
      "loss: 6.358807  [68760/315099]\n",
      "loss: 6.035782  [69010/315099]\n",
      "loss: 5.639473  [69260/315099]\n",
      "loss: 5.755698  [69510/315099]\n",
      "loss: 5.535351  [69760/315099]\n",
      "loss: 5.819628  [70010/315099]\n",
      "loss: 5.582124  [70260/315099]\n",
      "loss: 6.197246  [70510/315099]\n",
      "loss: 6.362013  [70760/315099]\n",
      "loss: 6.868249  [71010/315099]\n",
      "loss: 5.866876  [71260/315099]\n",
      "loss: 6.397640  [71510/315099]\n",
      "loss: 4.807277  [71760/315099]\n",
      "loss: 5.953871  [72010/315099]\n",
      "loss: 5.271280  [72260/315099]\n",
      "loss: 4.895589  [72510/315099]\n",
      "loss: 5.925586  [72760/315099]\n",
      "loss: 6.094544  [73010/315099]\n",
      "loss: 5.121772  [73260/315099]\n",
      "loss: 5.581753  [73510/315099]\n",
      "loss: 6.297199  [73760/315099]\n",
      "loss: 5.422612  [74010/315099]\n",
      "loss: 6.595703  [74260/315099]\n",
      "loss: 7.018112  [74510/315099]\n",
      "loss: 6.019577  [74760/315099]\n",
      "loss: 6.120173  [75010/315099]\n",
      "loss: 5.443231  [75260/315099]\n",
      "loss: 6.965390  [75510/315099]\n",
      "loss: 6.201171  [75760/315099]\n",
      "loss: 5.933018  [76010/315099]\n",
      "loss: 7.027198  [76260/315099]\n",
      "loss: 5.452323  [76510/315099]\n",
      "loss: 6.078895  [76760/315099]\n",
      "loss: 8.140017  [77010/315099]\n",
      "loss: 6.802747  [77260/315099]\n",
      "loss: 5.712483  [77510/315099]\n",
      "loss: 4.931773  [77760/315099]\n",
      "loss: 6.170609  [78010/315099]\n",
      "loss: 6.085574  [78260/315099]\n",
      "loss: 5.909451  [78510/315099]\n",
      "loss: 7.063474  [78760/315099]\n",
      "loss: 4.747799  [79010/315099]\n",
      "loss: 6.283391  [79260/315099]\n",
      "loss: 5.341361  [79510/315099]\n",
      "loss: 6.547315  [79760/315099]\n",
      "loss: 5.957505  [80010/315099]\n",
      "loss: 5.506722  [80260/315099]\n",
      "loss: 6.346148  [80510/315099]\n",
      "loss: 5.773963  [80760/315099]\n",
      "loss: 5.158876  [81010/315099]\n",
      "loss: 5.780993  [81260/315099]\n",
      "loss: 5.548455  [81510/315099]\n",
      "loss: 6.042210  [81760/315099]\n",
      "loss: 6.218714  [82010/315099]\n",
      "loss: 5.446444  [82260/315099]\n",
      "loss: 6.768571  [82510/315099]\n",
      "loss: 6.881845  [82760/315099]\n",
      "loss: 6.802659  [83010/315099]\n",
      "loss: 5.153852  [83260/315099]\n",
      "loss: 7.221280  [83510/315099]\n",
      "loss: 5.360476  [83760/315099]\n",
      "loss: 5.966457  [84010/315099]\n",
      "loss: 6.016133  [84260/315099]\n",
      "loss: 5.525178  [84510/315099]\n",
      "loss: 6.750653  [84760/315099]\n",
      "loss: 6.188722  [85010/315099]\n",
      "loss: 6.347430  [85260/315099]\n",
      "loss: 6.526650  [85510/315099]\n",
      "loss: 6.733869  [85760/315099]\n",
      "loss: 5.822277  [86010/315099]\n",
      "loss: 4.810419  [86260/315099]\n",
      "loss: 6.391074  [86510/315099]\n",
      "loss: 5.308946  [86760/315099]\n",
      "loss: 6.382850  [87010/315099]\n",
      "loss: 7.038903  [87260/315099]\n",
      "loss: 5.919680  [87510/315099]\n",
      "loss: 5.753605  [87760/315099]\n",
      "loss: 8.418091  [88010/315099]\n",
      "loss: 4.719451  [88260/315099]\n",
      "loss: 5.840329  [88510/315099]\n",
      "loss: 5.842758  [88760/315099]\n",
      "loss: 5.965701  [89010/315099]\n",
      "loss: 7.528478  [89260/315099]\n",
      "loss: 6.395222  [89510/315099]\n",
      "loss: 6.846075  [89760/315099]\n",
      "loss: 7.092234  [90010/315099]\n",
      "loss: 6.358292  [90260/315099]\n",
      "loss: 6.684252  [90510/315099]\n",
      "loss: 5.648091  [90760/315099]\n",
      "loss: 5.670061  [91010/315099]\n",
      "loss: 7.972070  [91260/315099]\n",
      "loss: 5.538938  [91510/315099]\n",
      "loss: 6.709425  [91760/315099]\n",
      "loss: 6.444780  [92010/315099]\n",
      "loss: 5.485383  [92260/315099]\n",
      "loss: 6.088404  [92510/315099]\n",
      "loss: 5.471287  [92760/315099]\n",
      "loss: 5.528436  [93010/315099]\n",
      "loss: 5.900563  [93260/315099]\n",
      "loss: 6.001045  [93510/315099]\n",
      "loss: 6.252456  [93760/315099]\n",
      "loss: 5.765204  [94010/315099]\n",
      "loss: 5.870448  [94260/315099]\n",
      "loss: 7.077107  [94510/315099]\n",
      "loss: 6.824991  [94760/315099]\n",
      "loss: 6.593060  [95010/315099]\n",
      "loss: 5.318902  [95260/315099]\n",
      "loss: 6.031938  [95510/315099]\n",
      "loss: 5.706251  [95760/315099]\n",
      "loss: 5.765891  [96010/315099]\n",
      "loss: 6.586493  [96260/315099]\n",
      "loss: 6.254425  [96510/315099]\n",
      "loss: 6.132074  [96760/315099]\n",
      "loss: 6.852343  [97010/315099]\n",
      "loss: 6.897898  [97260/315099]\n",
      "loss: 4.923660  [97510/315099]\n",
      "loss: 5.651332  [97760/315099]\n",
      "loss: 7.203871  [98010/315099]\n",
      "loss: 5.676937  [98260/315099]\n",
      "loss: 6.732127  [98510/315099]\n",
      "loss: 5.941319  [98760/315099]\n",
      "loss: 6.526938  [99010/315099]\n",
      "loss: 5.950873  [99260/315099]\n",
      "loss: 5.937411  [99510/315099]\n",
      "loss: 5.098145  [99760/315099]\n",
      "loss: 6.508032  [100010/315099]\n",
      "loss: 5.605692  [100260/315099]\n",
      "loss: 7.098006  [100510/315099]\n",
      "loss: 5.354487  [100760/315099]\n",
      "loss: 6.123338  [101010/315099]\n",
      "loss: 5.963040  [101260/315099]\n",
      "loss: 4.995970  [101510/315099]\n",
      "loss: 5.935362  [101760/315099]\n",
      "loss: 4.899624  [102010/315099]\n",
      "loss: 6.829161  [102260/315099]\n",
      "loss: 4.937682  [102510/315099]\n",
      "loss: 6.192595  [102760/315099]\n",
      "loss: 5.860827  [103010/315099]\n",
      "loss: 5.743275  [103260/315099]\n",
      "loss: 5.423337  [103510/315099]\n",
      "loss: 5.908286  [103760/315099]\n",
      "loss: 6.167014  [104010/315099]\n",
      "loss: 6.266210  [104260/315099]\n",
      "loss: 5.465711  [104510/315099]\n",
      "loss: 5.975344  [104760/315099]\n",
      "loss: 5.460128  [105010/315099]\n",
      "loss: 5.701896  [105260/315099]\n",
      "loss: 6.239647  [105510/315099]\n",
      "loss: 4.958507  [105760/315099]\n",
      "loss: 6.139578  [106010/315099]\n",
      "loss: 6.793029  [106260/315099]\n",
      "loss: 5.588919  [106510/315099]\n",
      "loss: 5.542663  [106760/315099]\n",
      "loss: 6.249469  [107010/315099]\n",
      "loss: 6.155557  [107260/315099]\n",
      "loss: 5.319986  [107510/315099]\n",
      "loss: 5.680537  [107760/315099]\n",
      "loss: 6.187813  [108010/315099]\n",
      "loss: 6.879102  [108260/315099]\n",
      "loss: 5.224962  [108510/315099]\n",
      "loss: 5.447500  [108760/315099]\n",
      "loss: 5.997999  [109010/315099]\n",
      "loss: 4.969302  [109260/315099]\n",
      "loss: 5.647571  [109510/315099]\n",
      "loss: 6.283885  [109760/315099]\n",
      "loss: 5.532613  [110010/315099]\n",
      "loss: 5.036155  [110260/315099]\n",
      "loss: 5.575932  [110510/315099]\n",
      "loss: 6.338016  [110760/315099]\n",
      "loss: 6.436910  [111010/315099]\n",
      "loss: 5.892282  [111260/315099]\n",
      "loss: 6.152297  [111510/315099]\n",
      "loss: 5.137301  [111760/315099]\n",
      "loss: 5.759363  [112010/315099]\n",
      "loss: 5.387547  [112260/315099]\n",
      "loss: 6.339977  [112510/315099]\n",
      "loss: 5.873000  [112760/315099]\n",
      "loss: 5.944575  [113010/315099]\n",
      "loss: 5.846149  [113260/315099]\n",
      "loss: 6.952660  [113510/315099]\n",
      "loss: 6.335005  [113760/315099]\n",
      "loss: 7.097629  [114010/315099]\n",
      "loss: 5.608196  [114260/315099]\n",
      "loss: 5.831839  [114510/315099]\n",
      "loss: 5.406821  [114760/315099]\n",
      "loss: 6.004643  [115010/315099]\n",
      "loss: 5.406142  [115260/315099]\n",
      "loss: 5.712476  [115510/315099]\n",
      "loss: 5.753477  [115760/315099]\n",
      "loss: 5.162044  [116010/315099]\n",
      "loss: 4.968561  [116260/315099]\n",
      "loss: 5.579286  [116510/315099]\n",
      "loss: 5.130612  [116760/315099]\n",
      "loss: 5.852531  [117010/315099]\n",
      "loss: 7.121839  [117260/315099]\n",
      "loss: 5.957973  [117510/315099]\n",
      "loss: 6.569367  [117760/315099]\n",
      "loss: 6.491842  [118010/315099]\n",
      "loss: 7.636992  [118260/315099]\n",
      "loss: 5.393348  [118510/315099]\n",
      "loss: 7.616009  [118760/315099]\n",
      "loss: 7.437261  [119010/315099]\n",
      "loss: 5.664037  [119260/315099]\n",
      "loss: 5.561161  [119510/315099]\n",
      "loss: 5.393708  [119760/315099]\n",
      "loss: 5.610578  [120010/315099]\n",
      "loss: 6.535081  [120260/315099]\n",
      "loss: 5.587800  [120510/315099]\n",
      "loss: 5.464120  [120760/315099]\n",
      "loss: 6.947389  [121010/315099]\n",
      "loss: 5.991510  [121260/315099]\n",
      "loss: 5.579810  [121510/315099]\n",
      "loss: 6.091299  [121760/315099]\n",
      "loss: 6.713496  [122010/315099]\n",
      "loss: 5.834293  [122260/315099]\n",
      "loss: 6.977242  [122510/315099]\n",
      "loss: 6.392076  [122760/315099]\n",
      "loss: 5.814024  [123010/315099]\n",
      "loss: 6.226526  [123260/315099]\n",
      "loss: 5.174291  [123510/315099]\n",
      "loss: 5.383612  [123760/315099]\n",
      "loss: 6.500712  [124010/315099]\n",
      "loss: 7.483246  [124260/315099]\n",
      "loss: 6.006558  [124510/315099]\n",
      "loss: 5.664038  [124760/315099]\n",
      "loss: 7.087914  [125010/315099]\n",
      "loss: 5.368708  [125260/315099]\n",
      "loss: 5.106091  [125510/315099]\n",
      "loss: 5.200161  [125760/315099]\n",
      "loss: 6.054335  [126010/315099]\n",
      "loss: 5.172247  [126260/315099]\n",
      "loss: 6.170079  [126510/315099]\n",
      "loss: 6.223494  [126760/315099]\n",
      "loss: 5.229220  [127010/315099]\n",
      "loss: 6.266894  [127260/315099]\n",
      "loss: 5.588375  [127510/315099]\n",
      "loss: 6.678792  [127760/315099]\n",
      "loss: 6.175347  [128010/315099]\n",
      "loss: 6.619166  [128260/315099]\n",
      "loss: 7.315817  [128510/315099]\n",
      "loss: 5.806821  [128760/315099]\n",
      "loss: 6.946023  [129010/315099]\n",
      "loss: 5.516628  [129260/315099]\n",
      "loss: 6.409896  [129510/315099]\n",
      "loss: 5.676413  [129760/315099]\n",
      "loss: 6.079880  [130010/315099]\n",
      "loss: 6.480060  [130260/315099]\n",
      "loss: 5.545454  [130510/315099]\n",
      "loss: 6.097534  [130760/315099]\n",
      "loss: 6.594798  [131010/315099]\n",
      "loss: 5.043423  [131260/315099]\n",
      "loss: 6.542425  [131510/315099]\n",
      "loss: 5.515206  [131760/315099]\n",
      "loss: 5.937448  [132010/315099]\n",
      "loss: 6.161673  [132260/315099]\n",
      "loss: 6.158253  [132510/315099]\n",
      "loss: 5.499549  [132760/315099]\n",
      "loss: 6.140358  [133010/315099]\n",
      "loss: 5.452831  [133260/315099]\n",
      "loss: 5.688003  [133510/315099]\n",
      "loss: 5.008345  [133760/315099]\n",
      "loss: 6.400794  [134010/315099]\n",
      "loss: 6.722477  [134260/315099]\n",
      "loss: 6.867674  [134510/315099]\n",
      "loss: 5.855430  [134760/315099]\n",
      "loss: 5.151580  [135010/315099]\n",
      "loss: 6.416318  [135260/315099]\n",
      "loss: 6.160026  [135510/315099]\n",
      "loss: 5.414257  [135760/315099]\n",
      "loss: 5.605966  [136010/315099]\n",
      "loss: 6.336802  [136260/315099]\n",
      "loss: 6.212218  [136510/315099]\n",
      "loss: 5.148708  [136760/315099]\n",
      "loss: 6.034974  [137010/315099]\n",
      "loss: 6.182054  [137260/315099]\n",
      "loss: 5.944758  [137510/315099]\n",
      "loss: 6.564832  [137760/315099]\n",
      "loss: 6.494609  [138010/315099]\n",
      "loss: 5.242294  [138260/315099]\n",
      "loss: 6.366356  [138510/315099]\n",
      "loss: 5.885829  [138760/315099]\n",
      "loss: 5.795964  [139010/315099]\n",
      "loss: 6.370835  [139260/315099]\n",
      "loss: 5.080476  [139510/315099]\n",
      "loss: 5.152287  [139760/315099]\n",
      "loss: 5.770404  [140010/315099]\n",
      "loss: 5.283988  [140260/315099]\n",
      "loss: 5.531926  [140510/315099]\n",
      "loss: 6.657506  [140760/315099]\n",
      "loss: 7.818282  [141010/315099]\n",
      "loss: 5.853226  [141260/315099]\n",
      "loss: 6.097510  [141510/315099]\n",
      "loss: 6.447053  [141760/315099]\n",
      "loss: 5.938794  [142010/315099]\n",
      "loss: 6.735587  [142260/315099]\n",
      "loss: 5.961897  [142510/315099]\n",
      "loss: 6.242259  [142760/315099]\n",
      "loss: 5.405172  [143010/315099]\n",
      "loss: 6.057558  [143260/315099]\n",
      "loss: 5.798100  [143510/315099]\n",
      "loss: 6.644523  [143760/315099]\n",
      "loss: 6.598842  [144010/315099]\n",
      "loss: 5.696046  [144260/315099]\n",
      "loss: 4.859883  [144510/315099]\n",
      "loss: 5.731123  [144760/315099]\n",
      "loss: 5.213253  [145010/315099]\n",
      "loss: 5.912687  [145260/315099]\n",
      "loss: 6.372476  [145510/315099]\n",
      "loss: 5.030006  [145760/315099]\n",
      "loss: 5.594094  [146010/315099]\n",
      "loss: 5.954432  [146260/315099]\n",
      "loss: 5.419962  [146510/315099]\n",
      "loss: 5.850994  [146760/315099]\n",
      "loss: 6.189565  [147010/315099]\n",
      "loss: 6.713427  [147260/315099]\n",
      "loss: 6.051894  [147510/315099]\n",
      "loss: 6.845447  [147760/315099]\n",
      "loss: 7.140310  [148010/315099]\n",
      "loss: 5.377216  [148260/315099]\n",
      "loss: 4.320005  [148510/315099]\n",
      "loss: 5.548769  [148760/315099]\n",
      "loss: 6.327335  [149010/315099]\n",
      "loss: 5.311706  [149260/315099]\n",
      "loss: 6.774109  [149510/315099]\n",
      "loss: 6.367599  [149760/315099]\n",
      "loss: 5.504609  [150010/315099]\n",
      "loss: 5.725046  [150260/315099]\n",
      "loss: 5.571850  [150510/315099]\n",
      "loss: 6.316153  [150760/315099]\n",
      "loss: 5.257195  [151010/315099]\n",
      "loss: 5.453149  [151260/315099]\n",
      "loss: 6.209062  [151510/315099]\n",
      "loss: 5.895844  [151760/315099]\n",
      "loss: 6.061409  [152010/315099]\n",
      "loss: 5.613837  [152260/315099]\n",
      "loss: 4.971141  [152510/315099]\n",
      "loss: 6.475562  [152760/315099]\n",
      "loss: 5.312560  [153010/315099]\n",
      "loss: 5.788553  [153260/315099]\n",
      "loss: 6.191630  [153510/315099]\n",
      "loss: 6.303748  [153760/315099]\n",
      "loss: 6.243070  [154010/315099]\n",
      "loss: 5.799023  [154260/315099]\n",
      "loss: 5.308971  [154510/315099]\n",
      "loss: 5.459024  [154760/315099]\n",
      "loss: 6.296098  [155010/315099]\n",
      "loss: 5.790011  [155260/315099]\n",
      "loss: 5.471433  [155510/315099]\n",
      "loss: 6.141634  [155760/315099]\n",
      "loss: 5.644553  [156010/315099]\n",
      "loss: 6.461358  [156260/315099]\n",
      "loss: 4.843930  [156510/315099]\n",
      "loss: 5.194970  [156760/315099]\n",
      "loss: 6.258315  [157010/315099]\n",
      "loss: 6.734328  [157260/315099]\n",
      "loss: 6.239660  [157510/315099]\n",
      "loss: 6.150975  [157760/315099]\n",
      "loss: 6.077546  [158010/315099]\n",
      "loss: 5.561934  [158260/315099]\n",
      "loss: 5.206304  [158510/315099]\n",
      "loss: 5.806802  [158760/315099]\n",
      "loss: 6.272257  [159010/315099]\n",
      "loss: 6.040259  [159260/315099]\n",
      "loss: 5.424285  [159510/315099]\n",
      "loss: 4.733426  [159760/315099]\n",
      "loss: 5.955687  [160010/315099]\n",
      "loss: 5.202413  [160260/315099]\n",
      "loss: 5.362353  [160510/315099]\n",
      "loss: 5.956729  [160760/315099]\n",
      "loss: 6.305487  [161010/315099]\n",
      "loss: 5.631151  [161260/315099]\n",
      "loss: 6.412342  [161510/315099]\n",
      "loss: 6.241100  [161760/315099]\n",
      "loss: 4.923421  [162010/315099]\n",
      "loss: 7.276718  [162260/315099]\n",
      "loss: 5.780499  [162510/315099]\n",
      "loss: 5.699490  [162760/315099]\n",
      "loss: 5.064696  [163010/315099]\n",
      "loss: 6.212089  [163260/315099]\n",
      "loss: 5.987736  [163510/315099]\n",
      "loss: 9.506907  [163760/315099]\n",
      "loss: 6.116996  [164010/315099]\n",
      "loss: 5.502731  [164260/315099]\n",
      "loss: 7.026829  [164510/315099]\n",
      "loss: 7.199577  [164760/315099]\n",
      "loss: 6.456346  [165010/315099]\n",
      "loss: 6.419938  [165260/315099]\n",
      "loss: 6.209770  [165510/315099]\n",
      "loss: 5.348065  [165760/315099]\n",
      "loss: 6.071266  [166010/315099]\n",
      "loss: 6.529262  [166260/315099]\n",
      "loss: 6.312660  [166510/315099]\n",
      "loss: 7.370084  [166760/315099]\n",
      "loss: 5.409317  [167010/315099]\n",
      "loss: 5.381505  [167260/315099]\n",
      "loss: 6.261923  [167510/315099]\n",
      "loss: 6.358609  [167760/315099]\n",
      "loss: 6.335811  [168010/315099]\n",
      "loss: 5.755950  [168260/315099]\n",
      "loss: 5.235425  [168510/315099]\n",
      "loss: 5.999473  [168760/315099]\n",
      "loss: 5.984556  [169010/315099]\n",
      "loss: 5.689415  [169260/315099]\n",
      "loss: 6.043643  [169510/315099]\n",
      "loss: 6.811996  [169760/315099]\n",
      "loss: 6.643547  [170010/315099]\n",
      "loss: 6.447636  [170260/315099]\n",
      "loss: 5.329751  [170510/315099]\n",
      "loss: 6.795537  [170760/315099]\n",
      "loss: 5.339976  [171010/315099]\n",
      "loss: 5.197536  [171260/315099]\n",
      "loss: 6.426230  [171510/315099]\n",
      "loss: 5.287800  [171760/315099]\n",
      "loss: 6.651145  [172010/315099]\n",
      "loss: 5.349259  [172260/315099]\n",
      "loss: 7.650744  [172510/315099]\n",
      "loss: 5.966360  [172760/315099]\n",
      "loss: 5.565836  [173010/315099]\n",
      "loss: 5.407300  [173260/315099]\n",
      "loss: 6.236679  [173510/315099]\n",
      "loss: 4.340558  [173760/315099]\n",
      "loss: 5.606850  [174010/315099]\n",
      "loss: 5.739854  [174260/315099]\n",
      "loss: 6.040031  [174510/315099]\n",
      "loss: 5.804122  [174760/315099]\n",
      "loss: 5.171212  [175010/315099]\n",
      "loss: 6.024763  [175260/315099]\n",
      "loss: 5.620144  [175510/315099]\n",
      "loss: 5.631115  [175760/315099]\n",
      "loss: 5.147739  [176010/315099]\n",
      "loss: 5.601344  [176260/315099]\n",
      "loss: 5.310820  [176510/315099]\n",
      "loss: 5.982138  [176760/315099]\n",
      "loss: 5.682343  [177010/315099]\n",
      "loss: 5.884318  [177260/315099]\n",
      "loss: 6.501890  [177510/315099]\n",
      "loss: 5.792900  [177760/315099]\n",
      "loss: 5.431376  [178010/315099]\n",
      "loss: 5.733272  [178260/315099]\n",
      "loss: 5.718031  [178510/315099]\n",
      "loss: 5.126384  [178760/315099]\n",
      "loss: 5.404972  [179010/315099]\n",
      "loss: 6.067118  [179260/315099]\n",
      "loss: 6.844769  [179510/315099]\n",
      "loss: 5.683480  [179760/315099]\n",
      "loss: 5.428518  [180010/315099]\n",
      "loss: 7.156934  [180260/315099]\n",
      "loss: 6.271346  [180510/315099]\n",
      "loss: 4.815328  [180760/315099]\n",
      "loss: 5.372754  [181010/315099]\n",
      "loss: 6.824048  [181260/315099]\n",
      "loss: 5.905616  [181510/315099]\n",
      "loss: 6.584610  [181760/315099]\n",
      "loss: 5.827193  [182010/315099]\n",
      "loss: 6.182856  [182260/315099]\n",
      "loss: 6.054534  [182510/315099]\n",
      "loss: 5.525672  [182760/315099]\n",
      "loss: 6.349967  [183010/315099]\n",
      "loss: 6.312506  [183260/315099]\n",
      "loss: 5.428282  [183510/315099]\n",
      "loss: 7.625259  [183760/315099]\n",
      "loss: 5.045060  [184010/315099]\n",
      "loss: 5.137349  [184260/315099]\n",
      "loss: 7.201416  [184510/315099]\n",
      "loss: 6.581149  [184760/315099]\n",
      "loss: 5.721410  [185010/315099]\n",
      "loss: 5.619939  [185260/315099]\n",
      "loss: 5.137540  [185510/315099]\n",
      "loss: 5.923864  [185760/315099]\n",
      "loss: 6.548337  [186010/315099]\n",
      "loss: 6.145571  [186260/315099]\n",
      "loss: 5.574861  [186510/315099]\n",
      "loss: 6.046681  [186760/315099]\n",
      "loss: 7.620482  [187010/315099]\n",
      "loss: 8.578427  [187260/315099]\n",
      "loss: 5.896174  [187510/315099]\n",
      "loss: 6.017241  [187760/315099]\n",
      "loss: 6.831522  [188010/315099]\n",
      "loss: 6.130075  [188260/315099]\n",
      "loss: 8.739676  [188510/315099]\n",
      "loss: 6.721702  [188760/315099]\n",
      "loss: 5.956733  [189010/315099]\n",
      "loss: 5.481277  [189260/315099]\n",
      "loss: 6.022654  [189510/315099]\n",
      "loss: 5.580108  [189760/315099]\n",
      "loss: 5.682828  [190010/315099]\n",
      "loss: 5.413922  [190260/315099]\n",
      "loss: 7.036012  [190510/315099]\n",
      "loss: 5.897182  [190760/315099]\n",
      "loss: 6.194240  [191010/315099]\n",
      "loss: 6.198532  [191260/315099]\n",
      "loss: 5.757696  [191510/315099]\n",
      "loss: 4.782317  [191760/315099]\n",
      "loss: 6.072211  [192010/315099]\n",
      "loss: 5.584639  [192260/315099]\n",
      "loss: 6.377830  [192510/315099]\n",
      "loss: 5.726936  [192760/315099]\n",
      "loss: 8.380438  [193010/315099]\n",
      "loss: 6.367650  [193260/315099]\n",
      "loss: 5.387544  [193510/315099]\n",
      "loss: 7.401299  [193760/315099]\n",
      "loss: 5.808728  [194010/315099]\n",
      "loss: 7.066556  [194260/315099]\n",
      "loss: 5.438827  [194510/315099]\n",
      "loss: 6.999030  [194760/315099]\n",
      "loss: 6.207367  [195010/315099]\n",
      "loss: 6.157487  [195260/315099]\n",
      "loss: 5.923426  [195510/315099]\n",
      "loss: 6.062841  [195760/315099]\n",
      "loss: 6.772459  [196010/315099]\n",
      "loss: 6.303057  [196260/315099]\n",
      "loss: 5.523570  [196510/315099]\n",
      "loss: 5.080444  [196760/315099]\n",
      "loss: 5.391011  [197010/315099]\n",
      "loss: 6.393977  [197260/315099]\n",
      "loss: 5.888230  [197510/315099]\n",
      "loss: 6.188210  [197760/315099]\n",
      "loss: 5.995449  [198010/315099]\n",
      "loss: 6.503325  [198260/315099]\n",
      "loss: 5.171249  [198510/315099]\n",
      "loss: 5.954363  [198760/315099]\n",
      "loss: 5.546262  [199010/315099]\n",
      "loss: 7.390106  [199260/315099]\n",
      "loss: 5.708663  [199510/315099]\n",
      "loss: 5.730493  [199760/315099]\n",
      "loss: 6.351973  [200010/315099]\n",
      "loss: 6.939384  [200260/315099]\n",
      "loss: 6.208497  [200510/315099]\n",
      "loss: 5.854611  [200760/315099]\n",
      "loss: 6.034529  [201010/315099]\n",
      "loss: 4.895901  [201260/315099]\n",
      "loss: 5.733388  [201510/315099]\n",
      "loss: 6.544825  [201760/315099]\n",
      "loss: 6.563515  [202010/315099]\n",
      "loss: 5.145391  [202260/315099]\n",
      "loss: 6.546894  [202510/315099]\n",
      "loss: 4.657095  [202760/315099]\n",
      "loss: 5.388830  [203010/315099]\n",
      "loss: 5.911332  [203260/315099]\n",
      "loss: 6.741825  [203510/315099]\n",
      "loss: 5.213332  [203760/315099]\n",
      "loss: 5.232937  [204010/315099]\n",
      "loss: 5.838334  [204260/315099]\n",
      "loss: 5.809816  [204510/315099]\n",
      "loss: 6.219398  [204760/315099]\n",
      "loss: 5.828517  [205010/315099]\n",
      "loss: 5.505369  [205260/315099]\n",
      "loss: 6.153207  [205510/315099]\n",
      "loss: 5.604473  [205760/315099]\n",
      "loss: 5.322740  [206010/315099]\n",
      "loss: 5.559980  [206260/315099]\n",
      "loss: 7.027508  [206510/315099]\n",
      "loss: 6.139581  [206760/315099]\n",
      "loss: 6.282476  [207010/315099]\n",
      "loss: 7.251906  [207260/315099]\n",
      "loss: 6.366961  [207510/315099]\n",
      "loss: 5.102533  [207760/315099]\n",
      "loss: 6.032434  [208010/315099]\n",
      "loss: 5.434426  [208260/315099]\n",
      "loss: 4.633008  [208510/315099]\n",
      "loss: 5.870001  [208760/315099]\n",
      "loss: 6.482317  [209010/315099]\n",
      "loss: 7.236785  [209260/315099]\n",
      "loss: 5.764459  [209510/315099]\n",
      "loss: 6.634350  [209760/315099]\n",
      "loss: 6.454759  [210010/315099]\n",
      "loss: 6.517377  [210260/315099]\n",
      "loss: 8.175554  [210510/315099]\n",
      "loss: 5.710760  [210760/315099]\n",
      "loss: 4.826190  [211010/315099]\n",
      "loss: 5.137885  [211260/315099]\n",
      "loss: 6.291619  [211510/315099]\n",
      "loss: 5.768519  [211760/315099]\n",
      "loss: 5.353941  [212010/315099]\n",
      "loss: 5.710860  [212260/315099]\n",
      "loss: 6.112473  [212510/315099]\n",
      "loss: 6.768260  [212760/315099]\n",
      "loss: 6.452658  [213010/315099]\n",
      "loss: 6.241375  [213260/315099]\n",
      "loss: 5.768540  [213510/315099]\n",
      "loss: 4.744346  [213760/315099]\n",
      "loss: 5.499362  [214010/315099]\n",
      "loss: 5.827736  [214260/315099]\n",
      "loss: 5.103022  [214510/315099]\n",
      "loss: 6.642772  [214760/315099]\n",
      "loss: 5.621139  [215010/315099]\n",
      "loss: 5.787707  [215260/315099]\n",
      "loss: 5.442622  [215510/315099]\n",
      "loss: 5.569542  [215760/315099]\n",
      "loss: 5.140463  [216010/315099]\n",
      "loss: 6.078455  [216260/315099]\n",
      "loss: 5.130540  [216510/315099]\n",
      "loss: 6.267082  [216760/315099]\n",
      "loss: 5.883216  [217010/315099]\n",
      "loss: 5.772669  [217260/315099]\n",
      "loss: 6.218581  [217510/315099]\n",
      "loss: 6.023251  [217760/315099]\n",
      "loss: 5.818717  [218010/315099]\n",
      "loss: 5.747037  [218260/315099]\n",
      "loss: 6.521463  [218510/315099]\n",
      "loss: 5.967550  [218760/315099]\n",
      "loss: 5.514944  [219010/315099]\n",
      "loss: 5.658433  [219260/315099]\n",
      "loss: 5.402278  [219510/315099]\n",
      "loss: 5.626709  [219760/315099]\n",
      "loss: 5.811806  [220010/315099]\n",
      "loss: 6.177560  [220260/315099]\n",
      "loss: 5.740506  [220510/315099]\n",
      "loss: 5.756892  [220760/315099]\n",
      "loss: 6.077446  [221010/315099]\n",
      "loss: 5.178252  [221260/315099]\n",
      "loss: 5.161543  [221510/315099]\n",
      "loss: 4.999756  [221760/315099]\n",
      "loss: 6.602633  [222010/315099]\n",
      "loss: 7.662420  [222260/315099]\n",
      "loss: 6.232614  [222510/315099]\n",
      "loss: 6.238349  [222760/315099]\n",
      "loss: 5.067566  [223010/315099]\n",
      "loss: 5.774744  [223260/315099]\n",
      "loss: 5.590279  [223510/315099]\n",
      "loss: 6.827197  [223760/315099]\n",
      "loss: 5.926421  [224010/315099]\n",
      "loss: 6.627180  [224260/315099]\n",
      "loss: 5.307590  [224510/315099]\n",
      "loss: 6.250790  [224760/315099]\n",
      "loss: 4.957544  [225010/315099]\n",
      "loss: 6.012090  [225260/315099]\n",
      "loss: 6.109600  [225510/315099]\n",
      "loss: 6.069248  [225760/315099]\n",
      "loss: 6.229030  [226010/315099]\n",
      "loss: 5.042548  [226260/315099]\n",
      "loss: 5.659610  [226510/315099]\n",
      "loss: 6.156816  [226760/315099]\n",
      "loss: 4.874099  [227010/315099]\n",
      "loss: 5.659163  [227260/315099]\n",
      "loss: 7.105013  [227510/315099]\n",
      "loss: 6.638330  [227760/315099]\n",
      "loss: 4.949835  [228010/315099]\n",
      "loss: 5.906694  [228260/315099]\n",
      "loss: 5.531138  [228510/315099]\n",
      "loss: 5.764303  [228760/315099]\n",
      "loss: 6.181156  [229010/315099]\n",
      "loss: 5.702598  [229260/315099]\n",
      "loss: 5.637277  [229510/315099]\n",
      "loss: 6.321807  [229760/315099]\n",
      "loss: 7.242367  [230010/315099]\n",
      "loss: 6.313931  [230260/315099]\n",
      "loss: 5.182975  [230510/315099]\n",
      "loss: 5.703499  [230760/315099]\n",
      "loss: 6.907416  [231010/315099]\n",
      "loss: 5.216186  [231260/315099]\n",
      "loss: 5.581848  [231510/315099]\n",
      "loss: 7.809188  [231760/315099]\n",
      "loss: 6.220407  [232010/315099]\n",
      "loss: 5.568715  [232260/315099]\n",
      "loss: 6.788861  [232510/315099]\n",
      "loss: 5.727890  [232760/315099]\n",
      "loss: 5.174810  [233010/315099]\n",
      "loss: 5.532515  [233260/315099]\n",
      "loss: 6.512227  [233510/315099]\n",
      "loss: 6.114366  [233760/315099]\n",
      "loss: 5.940413  [234010/315099]\n",
      "loss: 6.265042  [234260/315099]\n",
      "loss: 6.377103  [234510/315099]\n",
      "loss: 6.324163  [234760/315099]\n",
      "loss: 6.339830  [235010/315099]\n",
      "loss: 5.186635  [235260/315099]\n",
      "loss: 5.065214  [235510/315099]\n",
      "loss: 6.649759  [235760/315099]\n",
      "loss: 6.067883  [236010/315099]\n",
      "loss: 6.117400  [236260/315099]\n",
      "loss: 6.960046  [236510/315099]\n",
      "loss: 6.728081  [236760/315099]\n",
      "loss: 5.146922  [237010/315099]\n",
      "loss: 6.309564  [237260/315099]\n",
      "loss: 5.967670  [237510/315099]\n",
      "loss: 6.319075  [237760/315099]\n",
      "loss: 6.604435  [238010/315099]\n",
      "loss: 5.653670  [238260/315099]\n",
      "loss: 8.243413  [238510/315099]\n",
      "loss: 5.540957  [238760/315099]\n",
      "loss: 5.622596  [239010/315099]\n",
      "loss: 4.535353  [239260/315099]\n",
      "loss: 7.115906  [239510/315099]\n",
      "loss: 5.702235  [239760/315099]\n",
      "loss: 6.463656  [240010/315099]\n",
      "loss: 6.740954  [240260/315099]\n",
      "loss: 4.655284  [240510/315099]\n",
      "loss: 5.853149  [240760/315099]\n",
      "loss: 6.710900  [241010/315099]\n",
      "loss: 5.896297  [241260/315099]\n",
      "loss: 7.592388  [241510/315099]\n",
      "loss: 7.148848  [241760/315099]\n",
      "loss: 5.545866  [242010/315099]\n",
      "loss: 6.315012  [242260/315099]\n",
      "loss: 6.201098  [242510/315099]\n",
      "loss: 5.117129  [242760/315099]\n",
      "loss: 6.723649  [243010/315099]\n",
      "loss: 6.163067  [243260/315099]\n",
      "loss: 6.194785  [243510/315099]\n",
      "loss: 5.724605  [243760/315099]\n",
      "loss: 6.372703  [244010/315099]\n",
      "loss: 6.232802  [244260/315099]\n",
      "loss: 6.152588  [244510/315099]\n",
      "loss: 4.757754  [244760/315099]\n",
      "loss: 5.549356  [245010/315099]\n",
      "loss: 6.152423  [245260/315099]\n",
      "loss: 6.480818  [245510/315099]\n",
      "loss: 6.066721  [245760/315099]\n",
      "loss: 5.032523  [246010/315099]\n",
      "loss: 5.141092  [246260/315099]\n",
      "loss: 6.972802  [246510/315099]\n",
      "loss: 5.879795  [246760/315099]\n",
      "loss: 6.070848  [247010/315099]\n",
      "loss: 5.902083  [247260/315099]\n",
      "loss: 6.154757  [247510/315099]\n",
      "loss: 5.844090  [247760/315099]\n",
      "loss: 6.187335  [248010/315099]\n",
      "loss: 5.907363  [248260/315099]\n",
      "loss: 6.904943  [248510/315099]\n",
      "loss: 5.343843  [248760/315099]\n",
      "loss: 5.131610  [249010/315099]\n",
      "loss: 5.989882  [249260/315099]\n",
      "loss: 6.202607  [249510/315099]\n",
      "loss: 6.467290  [249760/315099]\n",
      "loss: 6.694257  [250010/315099]\n",
      "loss: 5.926936  [250260/315099]\n",
      "loss: 6.097468  [250510/315099]\n",
      "loss: 5.382612  [250760/315099]\n",
      "loss: 6.036314  [251010/315099]\n",
      "loss: 5.851559  [251260/315099]\n",
      "loss: 7.273042  [251510/315099]\n",
      "loss: 6.109315  [251760/315099]\n",
      "loss: 5.932061  [252010/315099]\n",
      "loss: 6.180854  [252260/315099]\n",
      "loss: 5.975637  [252510/315099]\n",
      "loss: 5.565554  [252760/315099]\n",
      "loss: 7.496032  [253010/315099]\n",
      "loss: 6.773655  [253260/315099]\n",
      "loss: 5.163156  [253510/315099]\n",
      "loss: 5.784123  [253760/315099]\n",
      "loss: 6.234640  [254010/315099]\n",
      "loss: 6.414272  [254260/315099]\n",
      "loss: 5.482491  [254510/315099]\n",
      "loss: 6.504286  [254760/315099]\n",
      "loss: 5.902988  [255010/315099]\n",
      "loss: 5.527977  [255260/315099]\n",
      "loss: 6.474053  [255510/315099]\n",
      "loss: 5.675531  [255760/315099]\n",
      "loss: 6.030303  [256010/315099]\n",
      "loss: 6.507552  [256260/315099]\n",
      "loss: 6.139063  [256510/315099]\n",
      "loss: 5.416818  [256760/315099]\n",
      "loss: 5.415858  [257010/315099]\n",
      "loss: 5.366523  [257260/315099]\n",
      "loss: 5.152712  [257510/315099]\n",
      "loss: 6.156792  [257760/315099]\n",
      "loss: 5.461057  [258010/315099]\n",
      "loss: 6.547862  [258260/315099]\n",
      "loss: 6.795008  [258510/315099]\n",
      "loss: 5.825032  [258760/315099]\n",
      "loss: 5.455908  [259010/315099]\n",
      "loss: 5.680637  [259260/315099]\n",
      "loss: 6.958010  [259510/315099]\n",
      "loss: 6.765929  [259760/315099]\n",
      "loss: 5.117407  [260010/315099]\n",
      "loss: 6.328482  [260260/315099]\n",
      "loss: 6.007449  [260510/315099]\n",
      "loss: 5.482947  [260760/315099]\n",
      "loss: 6.554930  [261010/315099]\n",
      "loss: 6.169349  [261260/315099]\n",
      "loss: 5.303861  [261510/315099]\n",
      "loss: 5.251292  [261760/315099]\n",
      "loss: 6.866889  [262010/315099]\n",
      "loss: 5.408193  [262260/315099]\n",
      "loss: 6.302845  [262510/315099]\n",
      "loss: 5.585238  [262760/315099]\n",
      "loss: 5.697529  [263010/315099]\n",
      "loss: 6.015729  [263260/315099]\n",
      "loss: 6.683706  [263510/315099]\n",
      "loss: 5.968755  [263760/315099]\n",
      "loss: 5.807034  [264010/315099]\n",
      "loss: 6.117218  [264260/315099]\n",
      "loss: 8.467474  [264510/315099]\n",
      "loss: 5.317670  [264760/315099]\n",
      "loss: 6.594546  [265010/315099]\n",
      "loss: 6.906309  [265260/315099]\n",
      "loss: 5.076087  [265510/315099]\n",
      "loss: 6.358616  [265760/315099]\n",
      "loss: 5.695356  [266010/315099]\n",
      "loss: 6.326876  [266260/315099]\n",
      "loss: 7.140880  [266510/315099]\n",
      "loss: 5.100961  [266760/315099]\n",
      "loss: 5.479777  [267010/315099]\n",
      "loss: 5.709930  [267260/315099]\n",
      "loss: 5.752206  [267510/315099]\n",
      "loss: 6.858490  [267760/315099]\n",
      "loss: 5.762738  [268010/315099]\n",
      "loss: 6.048407  [268260/315099]\n",
      "loss: 6.152267  [268510/315099]\n",
      "loss: 6.071406  [268760/315099]\n",
      "loss: 6.425075  [269010/315099]\n",
      "loss: 6.476369  [269260/315099]\n",
      "loss: 5.658092  [269510/315099]\n",
      "loss: 5.204885  [269760/315099]\n",
      "loss: 6.022287  [270010/315099]\n",
      "loss: 5.449445  [270260/315099]\n",
      "loss: 5.540642  [270510/315099]\n",
      "loss: 7.095884  [270760/315099]\n",
      "loss: 5.166788  [271010/315099]\n",
      "loss: 6.272714  [271260/315099]\n",
      "loss: 8.028879  [271510/315099]\n",
      "loss: 6.562041  [271760/315099]\n",
      "loss: 8.045593  [272010/315099]\n",
      "loss: 5.644331  [272260/315099]\n",
      "loss: 6.256204  [272510/315099]\n",
      "loss: 6.160425  [272760/315099]\n",
      "loss: 5.845466  [273010/315099]\n",
      "loss: 5.210989  [273260/315099]\n",
      "loss: 6.248635  [273510/315099]\n",
      "loss: 7.299725  [273760/315099]\n",
      "loss: 5.368688  [274010/315099]\n",
      "loss: 5.821399  [274260/315099]\n",
      "loss: 5.316689  [274510/315099]\n",
      "loss: 5.725710  [274760/315099]\n",
      "loss: 5.467684  [275010/315099]\n",
      "loss: 7.193588  [275260/315099]\n",
      "loss: 4.731925  [275510/315099]\n",
      "loss: 5.500189  [275760/315099]\n",
      "loss: 6.299321  [276010/315099]\n",
      "loss: 5.811446  [276260/315099]\n",
      "loss: 5.725497  [276510/315099]\n",
      "loss: 5.502095  [276760/315099]\n",
      "loss: 5.116269  [277010/315099]\n",
      "loss: 6.236056  [277260/315099]\n",
      "loss: 5.520129  [277510/315099]\n",
      "loss: 6.891104  [277760/315099]\n",
      "loss: 5.458879  [278010/315099]\n",
      "loss: 5.424787  [278260/315099]\n",
      "loss: 5.921815  [278510/315099]\n",
      "loss: 6.344541  [278760/315099]\n",
      "loss: 6.999330  [279010/315099]\n",
      "loss: 5.839633  [279260/315099]\n",
      "loss: 5.147038  [279510/315099]\n",
      "loss: 6.327977  [279760/315099]\n",
      "loss: 6.006525  [280010/315099]\n",
      "loss: 7.162035  [280260/315099]\n",
      "loss: 5.474187  [280510/315099]\n",
      "loss: 6.999557  [280760/315099]\n",
      "loss: 6.500734  [281010/315099]\n",
      "loss: 5.745627  [281260/315099]\n",
      "loss: 5.045525  [281510/315099]\n",
      "loss: 6.730682  [281760/315099]\n",
      "loss: 6.588384  [282010/315099]\n",
      "loss: 6.421483  [282260/315099]\n",
      "loss: 6.783153  [282510/315099]\n",
      "loss: 6.010732  [282760/315099]\n",
      "loss: 5.116776  [283010/315099]\n",
      "loss: 6.072055  [283260/315099]\n",
      "loss: 6.373202  [283510/315099]\n",
      "loss: 5.612506  [283760/315099]\n",
      "loss: 5.379678  [284010/315099]\n",
      "loss: 6.089285  [284260/315099]\n",
      "loss: 4.729385  [284510/315099]\n",
      "loss: 5.760993  [284760/315099]\n",
      "loss: 5.935254  [285010/315099]\n",
      "loss: 5.571286  [285260/315099]\n",
      "loss: 5.768962  [285510/315099]\n",
      "loss: 6.007351  [285760/315099]\n",
      "loss: 5.461584  [286010/315099]\n",
      "loss: 6.948212  [286260/315099]\n",
      "loss: 5.831288  [286510/315099]\n",
      "loss: 5.849005  [286760/315099]\n",
      "loss: 4.936955  [287010/315099]\n",
      "loss: 7.357335  [287260/315099]\n",
      "loss: 5.056684  [287510/315099]\n",
      "loss: 5.076730  [287760/315099]\n",
      "loss: 8.096776  [288010/315099]\n",
      "loss: 6.955839  [288260/315099]\n",
      "loss: 5.285338  [288510/315099]\n",
      "loss: 6.036438  [288760/315099]\n",
      "loss: 4.965034  [289010/315099]\n",
      "loss: 5.945070  [289260/315099]\n",
      "loss: 5.038825  [289510/315099]\n",
      "loss: 4.310055  [289760/315099]\n",
      "loss: 4.991331  [290010/315099]\n",
      "loss: 5.501996  [290260/315099]\n",
      "loss: 7.179736  [290510/315099]\n",
      "loss: 7.173940  [290760/315099]\n",
      "loss: 5.388723  [291010/315099]\n",
      "loss: 5.766064  [291260/315099]\n",
      "loss: 5.654341  [291510/315099]\n",
      "loss: 6.471145  [291760/315099]\n",
      "loss: 7.522470  [292010/315099]\n",
      "loss: 6.542305  [292260/315099]\n",
      "loss: 4.682539  [292510/315099]\n",
      "loss: 5.820430  [292760/315099]\n",
      "loss: 6.312655  [293010/315099]\n",
      "loss: 5.047414  [293260/315099]\n",
      "loss: 5.623720  [293510/315099]\n",
      "loss: 6.042264  [293760/315099]\n",
      "loss: 4.874980  [294010/315099]\n",
      "loss: 5.429937  [294260/315099]\n",
      "loss: 5.559698  [294510/315099]\n",
      "loss: 6.255766  [294760/315099]\n",
      "loss: 5.586390  [295010/315099]\n",
      "loss: 6.563867  [295260/315099]\n",
      "loss: 5.953090  [295510/315099]\n",
      "loss: 5.350274  [295760/315099]\n",
      "loss: 5.903266  [296010/315099]\n",
      "loss: 7.721487  [296260/315099]\n",
      "loss: 5.921723  [296510/315099]\n",
      "loss: 4.420146  [296760/315099]\n",
      "loss: 5.298715  [297010/315099]\n",
      "loss: 5.778420  [297260/315099]\n",
      "loss: 6.417253  [297510/315099]\n",
      "loss: 5.609860  [297760/315099]\n",
      "loss: 6.236263  [298010/315099]\n",
      "loss: 7.618639  [298260/315099]\n",
      "loss: 5.919081  [298510/315099]\n",
      "loss: 5.372392  [298760/315099]\n",
      "loss: 5.192617  [299010/315099]\n",
      "loss: 5.636652  [299260/315099]\n",
      "loss: 6.275153  [299510/315099]\n",
      "loss: 5.949433  [299760/315099]\n",
      "loss: 6.599719  [300010/315099]\n",
      "loss: 6.594968  [300260/315099]\n",
      "loss: 5.022796  [300510/315099]\n",
      "loss: 5.555352  [300760/315099]\n",
      "loss: 5.561202  [301010/315099]\n",
      "loss: 6.488168  [301260/315099]\n",
      "loss: 4.717663  [301510/315099]\n",
      "loss: 5.798790  [301760/315099]\n",
      "loss: 5.852563  [302010/315099]\n",
      "loss: 7.457879  [302260/315099]\n",
      "loss: 5.966950  [302510/315099]\n",
      "loss: 5.181646  [302760/315099]\n",
      "loss: 6.242793  [303010/315099]\n",
      "loss: 6.536927  [303260/315099]\n",
      "loss: 6.370360  [303510/315099]\n",
      "loss: 5.951692  [303760/315099]\n",
      "loss: 5.424790  [304010/315099]\n",
      "loss: 5.441261  [304260/315099]\n",
      "loss: 5.788596  [304510/315099]\n",
      "loss: 5.372752  [304760/315099]\n",
      "loss: 5.998582  [305010/315099]\n",
      "loss: 5.384304  [305260/315099]\n",
      "loss: 6.495972  [305510/315099]\n",
      "loss: 5.918817  [305760/315099]\n",
      "loss: 5.650370  [306010/315099]\n",
      "loss: 6.184801  [306260/315099]\n",
      "loss: 6.244662  [306510/315099]\n",
      "loss: 6.805747  [306760/315099]\n",
      "loss: 6.606859  [307010/315099]\n",
      "loss: 4.601025  [307260/315099]\n",
      "loss: 6.209035  [307510/315099]\n",
      "loss: 5.591689  [307760/315099]\n",
      "loss: 7.016538  [308010/315099]\n",
      "loss: 5.510699  [308260/315099]\n",
      "loss: 6.087369  [308510/315099]\n",
      "loss: 5.921727  [308760/315099]\n",
      "loss: 7.738265  [309010/315099]\n",
      "loss: 5.771997  [309260/315099]\n",
      "loss: 7.266172  [309510/315099]\n",
      "loss: 5.403657  [309760/315099]\n",
      "loss: 5.721218  [310010/315099]\n",
      "loss: 6.200198  [310260/315099]\n",
      "loss: 5.567726  [310510/315099]\n",
      "loss: 5.804010  [310760/315099]\n",
      "loss: 5.352501  [311010/315099]\n",
      "loss: 5.955483  [311260/315099]\n",
      "loss: 6.119956  [311510/315099]\n",
      "loss: 6.622115  [311760/315099]\n",
      "loss: 5.425977  [312010/315099]\n",
      "loss: 5.196490  [312260/315099]\n",
      "loss: 8.503057  [312510/315099]\n",
      "loss: 6.100834  [312760/315099]\n",
      "loss: 5.671437  [313010/315099]\n",
      "loss: 5.773529  [313260/315099]\n",
      "loss: 6.807414  [313510/315099]\n",
      "loss: 5.420957  [313760/315099]\n",
      "loss: 7.273681  [314010/315099]\n",
      "loss: 6.220836  [314260/315099]\n",
      "loss: 6.041592  [314510/315099]\n",
      "loss: 7.149047  [314760/315099]\n",
      "loss: 6.542142  [315010/315099]\n",
      "loss: 6.124801  [   10/315099]\n",
      "loss: 6.791067  [  260/315099]\n",
      "loss: 6.429469  [  510/315099]\n",
      "loss: 5.889536  [  760/315099]\n",
      "loss: 5.550080  [ 1010/315099]\n",
      "loss: 5.525484  [ 1260/315099]\n",
      "loss: 6.546046  [ 1510/315099]\n",
      "loss: 6.708685  [ 1760/315099]\n",
      "loss: 5.980062  [ 2010/315099]\n",
      "loss: 5.671710  [ 2260/315099]\n",
      "loss: 5.649215  [ 2510/315099]\n",
      "loss: 5.602850  [ 2760/315099]\n",
      "loss: 6.450063  [ 3010/315099]\n",
      "loss: 4.715261  [ 3260/315099]\n",
      "loss: 6.098137  [ 3510/315099]\n",
      "loss: 6.060582  [ 3760/315099]\n",
      "loss: 5.327292  [ 4010/315099]\n",
      "loss: 5.821493  [ 4260/315099]\n",
      "loss: 6.141000  [ 4510/315099]\n",
      "loss: 5.518853  [ 4760/315099]\n",
      "loss: 5.661411  [ 5010/315099]\n",
      "loss: 5.195020  [ 5260/315099]\n",
      "loss: 5.945943  [ 5510/315099]\n",
      "loss: 6.203173  [ 5760/315099]\n",
      "loss: 6.456006  [ 6010/315099]\n",
      "loss: 5.086371  [ 6260/315099]\n",
      "loss: 5.874241  [ 6510/315099]\n",
      "loss: 4.996791  [ 6760/315099]\n",
      "loss: 5.941461  [ 7010/315099]\n",
      "loss: 5.703958  [ 7260/315099]\n",
      "loss: 4.912272  [ 7510/315099]\n",
      "loss: 5.973123  [ 7760/315099]\n",
      "loss: 6.014379  [ 8010/315099]\n",
      "loss: 5.919173  [ 8260/315099]\n",
      "loss: 6.312035  [ 8510/315099]\n",
      "loss: 6.686085  [ 8760/315099]\n",
      "loss: 6.912065  [ 9010/315099]\n",
      "loss: 5.414736  [ 9260/315099]\n",
      "loss: 6.766724  [ 9510/315099]\n",
      "loss: 5.053041  [ 9760/315099]\n",
      "loss: 5.551859  [10010/315099]\n",
      "loss: 6.881189  [10260/315099]\n",
      "loss: 6.517002  [10510/315099]\n",
      "loss: 7.574476  [10760/315099]\n",
      "loss: 5.051287  [11010/315099]\n",
      "loss: 6.344047  [11260/315099]\n",
      "loss: 4.708070  [11510/315099]\n",
      "loss: 5.432243  [11760/315099]\n",
      "loss: 5.837041  [12010/315099]\n",
      "loss: 4.601581  [12260/315099]\n",
      "loss: 6.693071  [12510/315099]\n",
      "loss: 6.975285  [12760/315099]\n",
      "loss: 4.444467  [13010/315099]\n",
      "loss: 5.089510  [13260/315099]\n",
      "loss: 5.653110  [13510/315099]\n",
      "loss: 6.082614  [13760/315099]\n",
      "loss: 6.288672  [14010/315099]\n",
      "loss: 6.091043  [14260/315099]\n",
      "loss: 6.020551  [14510/315099]\n",
      "loss: 6.947443  [14760/315099]\n",
      "loss: 5.273175  [15010/315099]\n",
      "loss: 5.873003  [15260/315099]\n",
      "loss: 6.011060  [15510/315099]\n",
      "loss: 5.500294  [15760/315099]\n",
      "loss: 4.859214  [16010/315099]\n",
      "loss: 6.721194  [16260/315099]\n",
      "loss: 5.822146  [16510/315099]\n",
      "loss: 6.202466  [16760/315099]\n",
      "loss: 4.916449  [17010/315099]\n",
      "loss: 5.282290  [17260/315099]\n",
      "loss: 6.201620  [17510/315099]\n",
      "loss: 5.604675  [17760/315099]\n",
      "loss: 6.571749  [18010/315099]\n",
      "loss: 5.873323  [18260/315099]\n",
      "loss: 5.288091  [18510/315099]\n",
      "loss: 5.703522  [18760/315099]\n",
      "loss: 6.592678  [19010/315099]\n",
      "loss: 5.414697  [19260/315099]\n",
      "loss: 5.522883  [19510/315099]\n",
      "loss: 6.218140  [19760/315099]\n",
      "loss: 6.858921  [20010/315099]\n",
      "loss: 5.062932  [20260/315099]\n",
      "loss: 4.847213  [20510/315099]\n",
      "loss: 7.726192  [20760/315099]\n",
      "loss: 5.981942  [21010/315099]\n",
      "loss: 7.184380  [21260/315099]\n",
      "loss: 5.022349  [21510/315099]\n",
      "loss: 6.541306  [21760/315099]\n",
      "loss: 5.952729  [22010/315099]\n",
      "loss: 5.150341  [22260/315099]\n",
      "loss: 5.482403  [22510/315099]\n",
      "loss: 4.872428  [22760/315099]\n",
      "loss: 4.490791  [23010/315099]\n",
      "loss: 6.276300  [23260/315099]\n",
      "loss: 5.599759  [23510/315099]\n",
      "loss: 6.137582  [23760/315099]\n",
      "loss: 5.100583  [24010/315099]\n",
      "loss: 5.367026  [24260/315099]\n",
      "loss: 5.884968  [24510/315099]\n",
      "loss: 5.829509  [24760/315099]\n",
      "loss: 6.073542  [25010/315099]\n",
      "loss: 5.839235  [25260/315099]\n",
      "loss: 5.165936  [25510/315099]\n",
      "loss: 5.142732  [25760/315099]\n",
      "loss: 5.364053  [26010/315099]\n",
      "loss: 5.886569  [26260/315099]\n",
      "loss: 6.265460  [26510/315099]\n",
      "loss: 7.239986  [26760/315099]\n",
      "loss: 6.306555  [27010/315099]\n",
      "loss: 5.919135  [27260/315099]\n",
      "loss: 6.969680  [27510/315099]\n",
      "loss: 5.934785  [27760/315099]\n",
      "loss: 7.367743  [28010/315099]\n",
      "loss: 5.070758  [28260/315099]\n",
      "loss: 6.328479  [28510/315099]\n",
      "loss: 5.937465  [28760/315099]\n",
      "loss: 5.945491  [29010/315099]\n",
      "loss: 6.022418  [29260/315099]\n",
      "loss: 5.741586  [29510/315099]\n",
      "loss: 6.288425  [29760/315099]\n",
      "loss: 5.970148  [30010/315099]\n",
      "loss: 4.891083  [30260/315099]\n",
      "loss: 4.551330  [30510/315099]\n",
      "loss: 6.155239  [30760/315099]\n",
      "loss: 6.341086  [31010/315099]\n",
      "loss: 4.293715  [31260/315099]\n",
      "loss: 5.812447  [31510/315099]\n",
      "loss: 5.790068  [31760/315099]\n",
      "loss: 5.870183  [32010/315099]\n",
      "loss: 4.824949  [32260/315099]\n",
      "loss: 6.158387  [32510/315099]\n",
      "loss: 6.827409  [32760/315099]\n",
      "loss: 5.822227  [33010/315099]\n",
      "loss: 5.040164  [33260/315099]\n",
      "loss: 5.958934  [33510/315099]\n",
      "loss: 6.134629  [33760/315099]\n",
      "loss: 5.124844  [34010/315099]\n",
      "loss: 5.787956  [34260/315099]\n",
      "loss: 4.824960  [34510/315099]\n",
      "loss: 6.133412  [34760/315099]\n",
      "loss: 5.446192  [35010/315099]\n",
      "loss: 5.082653  [35260/315099]\n",
      "loss: 5.409067  [35510/315099]\n",
      "loss: 6.268094  [35760/315099]\n",
      "loss: 6.204302  [36010/315099]\n",
      "loss: 5.164809  [36260/315099]\n",
      "loss: 5.893347  [36510/315099]\n",
      "loss: 6.202168  [36760/315099]\n",
      "loss: 6.244617  [37010/315099]\n",
      "loss: 5.322330  [37260/315099]\n",
      "loss: 6.682094  [37510/315099]\n",
      "loss: 5.574584  [37760/315099]\n",
      "loss: 4.988673  [38010/315099]\n",
      "loss: 4.806462  [38260/315099]\n",
      "loss: 5.768878  [38510/315099]\n",
      "loss: 5.266891  [38760/315099]\n",
      "loss: 4.890607  [39010/315099]\n",
      "loss: 5.845893  [39260/315099]\n",
      "loss: 6.179702  [39510/315099]\n",
      "loss: 7.513267  [39760/315099]\n",
      "loss: 6.537679  [40010/315099]\n",
      "loss: 6.883168  [40260/315099]\n",
      "loss: 6.941503  [40510/315099]\n",
      "loss: 5.541940  [40760/315099]\n",
      "loss: 6.626203  [41010/315099]\n",
      "loss: 6.284999  [41260/315099]\n",
      "loss: 5.025518  [41510/315099]\n",
      "loss: 5.837179  [41760/315099]\n",
      "loss: 5.961404  [42010/315099]\n",
      "loss: 5.424072  [42260/315099]\n",
      "loss: 6.071691  [42510/315099]\n",
      "loss: 5.971316  [42760/315099]\n",
      "loss: 6.318194  [43010/315099]\n",
      "loss: 5.944936  [43260/315099]\n",
      "loss: 5.725489  [43510/315099]\n",
      "loss: 6.670084  [43760/315099]\n",
      "loss: 5.921999  [44010/315099]\n",
      "loss: 5.784998  [44260/315099]\n",
      "loss: 6.383014  [44510/315099]\n",
      "loss: 5.356744  [44760/315099]\n",
      "loss: 6.273191  [45010/315099]\n",
      "loss: 5.911871  [45260/315099]\n",
      "loss: 7.306046  [45510/315099]\n",
      "loss: 5.742343  [45760/315099]\n",
      "loss: 6.378333  [46010/315099]\n",
      "loss: 7.336638  [46260/315099]\n",
      "loss: 7.144223  [46510/315099]\n",
      "loss: 6.561526  [46760/315099]\n",
      "loss: 5.922397  [47010/315099]\n",
      "loss: 5.256673  [47260/315099]\n",
      "loss: 6.751318  [47510/315099]\n",
      "loss: 6.689917  [47760/315099]\n",
      "loss: 5.260603  [48010/315099]\n",
      "loss: 5.137548  [48260/315099]\n",
      "loss: 6.267285  [48510/315099]\n",
      "loss: 5.790158  [48760/315099]\n",
      "loss: 7.798399  [49010/315099]\n",
      "loss: 5.058738  [49260/315099]\n",
      "loss: 5.292853  [49510/315099]\n",
      "loss: 6.071446  [49760/315099]\n",
      "loss: 5.316671  [50010/315099]\n",
      "loss: 5.798512  [50260/315099]\n",
      "loss: 5.462276  [50510/315099]\n",
      "loss: 5.488326  [50760/315099]\n",
      "loss: 5.002509  [51010/315099]\n",
      "loss: 4.830048  [51260/315099]\n",
      "loss: 5.369424  [51510/315099]\n",
      "loss: 5.469601  [51760/315099]\n",
      "loss: 6.221200  [52010/315099]\n",
      "loss: 5.191172  [52260/315099]\n",
      "loss: 5.695305  [52510/315099]\n",
      "loss: 5.730657  [52760/315099]\n",
      "loss: 5.846981  [53010/315099]\n",
      "loss: 5.770802  [53260/315099]\n",
      "loss: 5.516769  [53510/315099]\n",
      "loss: 5.412698  [53760/315099]\n",
      "loss: 6.135873  [54010/315099]\n",
      "loss: 5.499917  [54260/315099]\n",
      "loss: 6.132136  [54510/315099]\n",
      "loss: 5.571899  [54760/315099]\n",
      "loss: 5.419883  [55010/315099]\n",
      "loss: 5.911880  [55260/315099]\n",
      "loss: 5.386959  [55510/315099]\n",
      "loss: 6.124138  [55760/315099]\n",
      "loss: 6.156652  [56010/315099]\n",
      "loss: 7.180602  [56260/315099]\n",
      "loss: 5.325507  [56510/315099]\n",
      "loss: 6.597842  [56760/315099]\n",
      "loss: 6.093366  [57010/315099]\n",
      "loss: 7.041060  [57260/315099]\n",
      "loss: 7.099461  [57510/315099]\n",
      "loss: 6.127567  [57760/315099]\n",
      "loss: 7.337078  [58010/315099]\n",
      "loss: 7.535559  [58260/315099]\n",
      "loss: 4.713436  [58510/315099]\n",
      "loss: 5.300413  [58760/315099]\n",
      "loss: 6.061862  [59010/315099]\n",
      "loss: 6.147099  [59260/315099]\n",
      "loss: 5.974875  [59510/315099]\n",
      "loss: 7.060484  [59760/315099]\n",
      "loss: 5.650818  [60010/315099]\n",
      "loss: 6.734672  [60260/315099]\n",
      "loss: 7.351219  [60510/315099]\n",
      "loss: 5.420391  [60760/315099]\n",
      "loss: 5.390007  [61010/315099]\n",
      "loss: 5.960421  [61260/315099]\n",
      "loss: 5.198712  [61510/315099]\n",
      "loss: 5.933276  [61760/315099]\n",
      "loss: 5.842050  [62010/315099]\n",
      "loss: 6.218837  [62260/315099]\n",
      "loss: 6.560473  [62510/315099]\n",
      "loss: 4.917899  [62760/315099]\n",
      "loss: 5.363162  [63010/315099]\n",
      "loss: 5.913388  [63260/315099]\n",
      "loss: 5.267167  [63510/315099]\n",
      "loss: 6.992219  [63760/315099]\n",
      "loss: 6.838619  [64010/315099]\n",
      "loss: 5.878327  [64260/315099]\n",
      "loss: 5.441896  [64510/315099]\n",
      "loss: 5.769410  [64760/315099]\n",
      "loss: 5.374812  [65010/315099]\n",
      "loss: 5.730933  [65260/315099]\n",
      "loss: 5.471590  [65510/315099]\n",
      "loss: 5.707906  [65760/315099]\n",
      "loss: 6.282767  [66010/315099]\n",
      "loss: 6.847182  [66260/315099]\n",
      "loss: 7.029916  [66510/315099]\n",
      "loss: 5.878184  [66760/315099]\n",
      "loss: 6.027669  [67010/315099]\n",
      "loss: 5.547340  [67260/315099]\n",
      "loss: 5.907414  [67510/315099]\n",
      "loss: 6.843754  [67760/315099]\n",
      "loss: 5.793035  [68010/315099]\n",
      "loss: 5.740053  [68260/315099]\n",
      "loss: 6.046164  [68510/315099]\n",
      "loss: 6.361321  [68760/315099]\n",
      "loss: 5.076016  [69010/315099]\n",
      "loss: 6.963518  [69260/315099]\n",
      "loss: 5.990638  [69510/315099]\n",
      "loss: 5.667285  [69760/315099]\n",
      "loss: 5.729033  [70010/315099]\n",
      "loss: 5.715576  [70260/315099]\n",
      "loss: 6.172555  [70510/315099]\n",
      "loss: 5.468331  [70760/315099]\n",
      "loss: 4.840615  [71010/315099]\n",
      "loss: 6.026555  [71260/315099]\n",
      "loss: 6.738572  [71510/315099]\n",
      "loss: 5.881683  [71760/315099]\n",
      "loss: 5.404455  [72010/315099]\n",
      "loss: 5.287162  [72260/315099]\n",
      "loss: 7.545403  [72510/315099]\n",
      "loss: 4.285855  [72760/315099]\n",
      "loss: 5.990743  [73010/315099]\n",
      "loss: 7.300414  [73260/315099]\n",
      "loss: 6.014378  [73510/315099]\n",
      "loss: 5.929722  [73760/315099]\n",
      "loss: 7.071802  [74010/315099]\n",
      "loss: 6.668939  [74260/315099]\n",
      "loss: 6.208970  [74510/315099]\n",
      "loss: 7.604692  [74760/315099]\n",
      "loss: 6.595053  [75010/315099]\n",
      "loss: 5.403258  [75260/315099]\n",
      "loss: 4.690223  [75510/315099]\n",
      "loss: 6.146247  [75760/315099]\n",
      "loss: 5.123943  [76010/315099]\n",
      "loss: 7.327212  [76260/315099]\n",
      "loss: 7.447074  [76510/315099]\n",
      "loss: 5.842373  [76760/315099]\n",
      "loss: 6.470670  [77010/315099]\n",
      "loss: 7.606267  [77260/315099]\n",
      "loss: 5.828971  [77510/315099]\n",
      "loss: 5.512231  [77760/315099]\n",
      "loss: 4.942878  [78010/315099]\n",
      "loss: 5.126195  [78260/315099]\n",
      "loss: 5.655608  [78510/315099]\n",
      "loss: 5.569186  [78760/315099]\n",
      "loss: 5.349092  [79010/315099]\n",
      "loss: 6.772477  [79260/315099]\n",
      "loss: 5.294861  [79510/315099]\n",
      "loss: 6.120301  [79760/315099]\n",
      "loss: 7.184699  [80010/315099]\n",
      "loss: 6.285332  [80260/315099]\n",
      "loss: 5.421976  [80510/315099]\n",
      "loss: 5.710858  [80760/315099]\n",
      "loss: 5.556530  [81010/315099]\n",
      "loss: 5.936557  [81260/315099]\n",
      "loss: 4.858773  [81510/315099]\n",
      "loss: 5.948894  [81760/315099]\n",
      "loss: 6.099138  [82010/315099]\n",
      "loss: 5.235214  [82260/315099]\n",
      "loss: 6.582428  [82510/315099]\n",
      "loss: 6.186452  [82760/315099]\n",
      "loss: 6.555779  [83010/315099]\n",
      "loss: 5.388563  [83260/315099]\n",
      "loss: 5.813822  [83510/315099]\n",
      "loss: 6.040296  [83760/315099]\n",
      "loss: 6.369669  [84010/315099]\n",
      "loss: 5.815567  [84260/315099]\n",
      "loss: 5.521418  [84510/315099]\n",
      "loss: 6.208436  [84760/315099]\n",
      "loss: 6.264532  [85010/315099]\n",
      "loss: 7.649888  [85260/315099]\n",
      "loss: 6.889934  [85510/315099]\n",
      "loss: 6.008318  [85760/315099]\n",
      "loss: 6.346335  [86010/315099]\n",
      "loss: 6.683642  [86260/315099]\n",
      "loss: 5.808284  [86510/315099]\n",
      "loss: 7.051586  [86760/315099]\n",
      "loss: 7.309125  [87010/315099]\n",
      "loss: 5.149457  [87260/315099]\n",
      "loss: 5.387723  [87510/315099]\n",
      "loss: 6.565763  [87760/315099]\n",
      "loss: 5.494223  [88010/315099]\n",
      "loss: 5.374187  [88260/315099]\n",
      "loss: 5.369936  [88510/315099]\n",
      "loss: 5.851151  [88760/315099]\n",
      "loss: 7.139829  [89010/315099]\n",
      "loss: 5.337813  [89260/315099]\n",
      "loss: 5.901177  [89510/315099]\n",
      "loss: 5.494087  [89760/315099]\n",
      "loss: 5.181190  [90010/315099]\n",
      "loss: 6.260320  [90260/315099]\n",
      "loss: 5.816594  [90510/315099]\n",
      "loss: 6.690141  [90760/315099]\n",
      "loss: 5.647871  [91010/315099]\n",
      "loss: 6.513232  [91260/315099]\n",
      "loss: 6.086818  [91510/315099]\n",
      "loss: 6.301345  [91760/315099]\n",
      "loss: 5.586144  [92010/315099]\n",
      "loss: 5.807565  [92260/315099]\n",
      "loss: 4.938789  [92510/315099]\n",
      "loss: 6.646956  [92760/315099]\n",
      "loss: 5.932828  [93010/315099]\n",
      "loss: 5.219294  [93260/315099]\n",
      "loss: 5.939969  [93510/315099]\n",
      "loss: 5.160639  [93760/315099]\n",
      "loss: 5.148311  [94010/315099]\n",
      "loss: 5.972603  [94260/315099]\n",
      "loss: 6.384490  [94510/315099]\n",
      "loss: 5.730541  [94760/315099]\n",
      "loss: 4.703351  [95010/315099]\n",
      "loss: 6.373841  [95260/315099]\n",
      "loss: 7.178216  [95510/315099]\n",
      "loss: 5.626550  [95760/315099]\n",
      "loss: 4.864493  [96010/315099]\n",
      "loss: 6.137204  [96260/315099]\n",
      "loss: 5.874052  [96510/315099]\n",
      "loss: 4.957972  [96760/315099]\n",
      "loss: 6.104582  [97010/315099]\n",
      "loss: 5.236605  [97260/315099]\n",
      "loss: 5.212921  [97510/315099]\n",
      "loss: 5.218035  [97760/315099]\n",
      "loss: 6.427684  [98010/315099]\n",
      "loss: 6.727711  [98260/315099]\n",
      "loss: 5.313031  [98510/315099]\n",
      "loss: 6.320756  [98760/315099]\n",
      "loss: 5.682565  [99010/315099]\n",
      "loss: 5.241164  [99260/315099]\n",
      "loss: 5.924477  [99510/315099]\n",
      "loss: 6.041426  [99760/315099]\n",
      "loss: 4.990851  [100010/315099]\n",
      "loss: 6.314543  [100260/315099]\n",
      "loss: 5.401102  [100510/315099]\n",
      "loss: 6.207938  [100760/315099]\n",
      "loss: 6.230794  [101010/315099]\n",
      "loss: 5.317949  [101260/315099]\n",
      "loss: 6.698245  [101510/315099]\n",
      "loss: 6.569211  [101760/315099]\n",
      "loss: 6.285697  [102010/315099]\n",
      "loss: 5.289527  [102260/315099]\n",
      "loss: 5.316167  [102510/315099]\n",
      "loss: 6.028383  [102760/315099]\n",
      "loss: 6.280395  [103010/315099]\n",
      "loss: 5.918088  [103260/315099]\n",
      "loss: 5.385931  [103510/315099]\n",
      "loss: 6.257979  [103760/315099]\n",
      "loss: 5.494009  [104010/315099]\n",
      "loss: 6.502655  [104260/315099]\n",
      "loss: 5.513968  [104510/315099]\n",
      "loss: 5.727146  [104760/315099]\n",
      "loss: 5.801672  [105010/315099]\n",
      "loss: 5.169729  [105260/315099]\n",
      "loss: 6.648863  [105510/315099]\n",
      "loss: 4.881890  [105760/315099]\n",
      "loss: 6.575830  [106010/315099]\n",
      "loss: 6.100643  [106260/315099]\n",
      "loss: 5.656329  [106510/315099]\n",
      "loss: 5.584766  [106760/315099]\n",
      "loss: 7.460101  [107010/315099]\n",
      "loss: 7.165964  [107260/315099]\n",
      "loss: 5.439223  [107510/315099]\n",
      "loss: 5.396445  [107760/315099]\n",
      "loss: 6.134058  [108010/315099]\n",
      "loss: 4.902833  [108260/315099]\n",
      "loss: 5.726108  [108510/315099]\n",
      "loss: 5.678463  [108760/315099]\n",
      "loss: 6.299841  [109010/315099]\n",
      "loss: 6.002442  [109260/315099]\n",
      "loss: 5.827468  [109510/315099]\n",
      "loss: 6.231932  [109760/315099]\n",
      "loss: 5.525345  [110010/315099]\n",
      "loss: 6.168281  [110260/315099]\n",
      "loss: 5.877097  [110510/315099]\n",
      "loss: 6.801172  [110760/315099]\n",
      "loss: 5.399576  [111010/315099]\n",
      "loss: 5.299920  [111260/315099]\n",
      "loss: 5.209207  [111510/315099]\n",
      "loss: 5.192446  [111760/315099]\n",
      "loss: 6.366591  [112010/315099]\n",
      "loss: 6.709452  [112260/315099]\n",
      "loss: 7.726523  [112510/315099]\n",
      "loss: 5.067155  [112760/315099]\n",
      "loss: 6.324685  [113010/315099]\n",
      "loss: 6.185961  [113260/315099]\n",
      "loss: 5.568633  [113510/315099]\n",
      "loss: 5.658141  [113760/315099]\n",
      "loss: 5.861897  [114010/315099]\n",
      "loss: 6.376256  [114260/315099]\n",
      "loss: 6.009606  [114510/315099]\n",
      "loss: 5.980056  [114760/315099]\n",
      "loss: 5.713325  [115010/315099]\n",
      "loss: 5.744377  [115260/315099]\n",
      "loss: 6.021328  [115510/315099]\n",
      "loss: 5.650076  [115760/315099]\n",
      "loss: 6.211170  [116010/315099]\n",
      "loss: 5.801912  [116260/315099]\n",
      "loss: 5.520194  [116510/315099]\n",
      "loss: 5.797777  [116760/315099]\n",
      "loss: 5.849490  [117010/315099]\n",
      "loss: 6.132848  [117260/315099]\n",
      "loss: 6.160809  [117510/315099]\n",
      "loss: 5.856003  [117760/315099]\n",
      "loss: 5.681678  [118010/315099]\n",
      "loss: 5.415792  [118260/315099]\n",
      "loss: 5.411044  [118510/315099]\n",
      "loss: 6.524139  [118760/315099]\n",
      "loss: 5.728009  [119010/315099]\n",
      "loss: 7.814158  [119260/315099]\n",
      "loss: 5.921366  [119510/315099]\n",
      "loss: 4.369039  [119760/315099]\n",
      "loss: 5.750068  [120010/315099]\n",
      "loss: 5.846411  [120260/315099]\n",
      "loss: 6.303960  [120510/315099]\n",
      "loss: 6.829511  [120760/315099]\n",
      "loss: 6.298801  [121010/315099]\n",
      "loss: 6.797422  [121260/315099]\n",
      "loss: 5.702247  [121510/315099]\n",
      "loss: 5.475430  [121760/315099]\n",
      "loss: 6.421956  [122010/315099]\n",
      "loss: 6.116418  [122260/315099]\n",
      "loss: 6.636434  [122510/315099]\n",
      "loss: 6.638683  [122760/315099]\n",
      "loss: 6.093335  [123010/315099]\n",
      "loss: 5.875360  [123260/315099]\n",
      "loss: 7.659833  [123510/315099]\n",
      "loss: 4.504719  [123760/315099]\n",
      "loss: 5.305013  [124010/315099]\n",
      "loss: 6.252522  [124260/315099]\n",
      "loss: 5.642590  [124510/315099]\n",
      "loss: 4.805664  [124760/315099]\n",
      "loss: 6.135204  [125010/315099]\n",
      "loss: 6.318416  [125260/315099]\n",
      "loss: 6.344434  [125510/315099]\n",
      "loss: 4.877892  [125760/315099]\n",
      "loss: 4.635590  [126010/315099]\n",
      "loss: 6.036759  [126260/315099]\n",
      "loss: 5.958812  [126510/315099]\n",
      "loss: 5.796228  [126760/315099]\n",
      "loss: 6.976701  [127010/315099]\n",
      "loss: 5.432275  [127260/315099]\n",
      "loss: 6.477111  [127510/315099]\n",
      "loss: 7.133623  [127760/315099]\n",
      "loss: 5.726056  [128010/315099]\n",
      "loss: 5.825959  [128260/315099]\n",
      "loss: 5.176005  [128510/315099]\n",
      "loss: 5.710425  [128760/315099]\n",
      "loss: 5.072325  [129010/315099]\n",
      "loss: 5.115394  [129260/315099]\n",
      "loss: 6.063683  [129510/315099]\n",
      "loss: 6.385758  [129760/315099]\n",
      "loss: 5.423032  [130010/315099]\n",
      "loss: 5.112927  [130260/315099]\n",
      "loss: 4.527709  [130510/315099]\n",
      "loss: 6.020255  [130760/315099]\n",
      "loss: 5.694124  [131010/315099]\n",
      "loss: 7.877385  [131260/315099]\n",
      "loss: 5.245475  [131510/315099]\n",
      "loss: 5.709817  [131760/315099]\n",
      "loss: 7.021317  [132010/315099]\n",
      "loss: 6.358193  [132260/315099]\n",
      "loss: 5.342710  [132510/315099]\n",
      "loss: 5.436337  [132760/315099]\n",
      "loss: 5.970241  [133010/315099]\n",
      "loss: 6.415024  [133260/315099]\n",
      "loss: 5.999375  [133510/315099]\n",
      "loss: 4.400977  [133760/315099]\n",
      "loss: 5.308502  [134010/315099]\n",
      "loss: 5.995572  [134260/315099]\n",
      "loss: 6.395517  [134510/315099]\n",
      "loss: 4.948770  [134760/315099]\n",
      "loss: 5.834413  [135010/315099]\n",
      "loss: 6.453102  [135260/315099]\n",
      "loss: 6.091743  [135510/315099]\n",
      "loss: 4.650535  [135760/315099]\n",
      "loss: 5.697556  [136010/315099]\n",
      "loss: 5.639823  [136260/315099]\n",
      "loss: 5.709899  [136510/315099]\n",
      "loss: 7.992017  [136760/315099]\n",
      "loss: 5.533398  [137010/315099]\n",
      "loss: 5.432611  [137260/315099]\n",
      "loss: 5.994813  [137510/315099]\n",
      "loss: 6.037524  [137760/315099]\n",
      "loss: 5.639487  [138010/315099]\n",
      "loss: 4.791126  [138260/315099]\n",
      "loss: 6.299026  [138510/315099]\n",
      "loss: 5.242465  [138760/315099]\n",
      "loss: 5.490401  [139010/315099]\n",
      "loss: 5.189006  [139260/315099]\n",
      "loss: 5.778489  [139510/315099]\n",
      "loss: 6.078172  [139760/315099]\n",
      "loss: 5.902442  [140010/315099]\n",
      "loss: 7.603026  [140260/315099]\n",
      "loss: 7.824938  [140510/315099]\n",
      "loss: 5.737981  [140760/315099]\n",
      "loss: 5.394855  [141010/315099]\n",
      "loss: 5.686623  [141260/315099]\n",
      "loss: 6.305364  [141510/315099]\n",
      "loss: 6.075564  [141760/315099]\n",
      "loss: 5.437708  [142010/315099]\n",
      "loss: 5.705610  [142260/315099]\n",
      "loss: 6.658331  [142510/315099]\n",
      "loss: 4.859257  [142760/315099]\n",
      "loss: 5.929973  [143010/315099]\n",
      "loss: 5.131923  [143260/315099]\n",
      "loss: 5.431653  [143510/315099]\n",
      "loss: 5.966146  [143760/315099]\n",
      "loss: 6.196424  [144010/315099]\n",
      "loss: 6.431297  [144260/315099]\n",
      "loss: 5.426051  [144510/315099]\n",
      "loss: 6.569354  [144760/315099]\n",
      "loss: 5.078371  [145010/315099]\n",
      "loss: 6.055287  [145260/315099]\n",
      "loss: 5.143185  [145510/315099]\n",
      "loss: 5.761077  [145760/315099]\n",
      "loss: 5.943704  [146010/315099]\n",
      "loss: 6.379484  [146260/315099]\n",
      "loss: 6.206304  [146510/315099]\n",
      "loss: 6.222304  [146760/315099]\n",
      "loss: 5.990808  [147010/315099]\n",
      "loss: 5.864387  [147260/315099]\n",
      "loss: 5.754500  [147510/315099]\n",
      "loss: 5.965631  [147760/315099]\n",
      "loss: 5.234487  [148010/315099]\n",
      "loss: 5.802508  [148260/315099]\n",
      "loss: 4.376466  [148510/315099]\n",
      "loss: 5.700714  [148760/315099]\n",
      "loss: 5.780802  [149010/315099]\n",
      "loss: 5.386180  [149260/315099]\n",
      "loss: 5.417323  [149510/315099]\n",
      "loss: 4.447026  [149760/315099]\n",
      "loss: 6.169912  [150010/315099]\n",
      "loss: 5.818941  [150260/315099]\n",
      "loss: 5.785836  [150510/315099]\n",
      "loss: 5.948581  [150760/315099]\n",
      "loss: 5.522691  [151010/315099]\n",
      "loss: 6.005239  [151260/315099]\n",
      "loss: 5.746441  [151510/315099]\n",
      "loss: 4.906149  [151760/315099]\n",
      "loss: 5.305997  [152010/315099]\n",
      "loss: 5.825117  [152260/315099]\n",
      "loss: 5.335183  [152510/315099]\n",
      "loss: 7.095708  [152760/315099]\n",
      "loss: 8.127028  [153010/315099]\n",
      "loss: 4.997430  [153260/315099]\n",
      "loss: 5.591331  [153510/315099]\n",
      "loss: 5.244355  [153760/315099]\n",
      "loss: 7.368335  [154010/315099]\n",
      "loss: 5.187967  [154260/315099]\n",
      "loss: 6.006994  [154510/315099]\n",
      "loss: 5.297575  [154760/315099]\n",
      "loss: 5.881700  [155010/315099]\n",
      "loss: 7.317319  [155260/315099]\n",
      "loss: 6.080354  [155510/315099]\n",
      "loss: 6.718237  [155760/315099]\n",
      "loss: 6.475820  [156010/315099]\n",
      "loss: 6.081752  [156260/315099]\n",
      "loss: 7.595295  [156510/315099]\n",
      "loss: 6.197011  [156760/315099]\n",
      "loss: 5.565593  [157010/315099]\n",
      "loss: 7.280005  [157260/315099]\n",
      "loss: 5.344398  [157510/315099]\n",
      "loss: 5.780472  [157760/315099]\n",
      "loss: 6.249068  [158010/315099]\n",
      "loss: 5.427979  [158260/315099]\n",
      "loss: 8.481524  [158510/315099]\n",
      "loss: 4.553516  [158760/315099]\n",
      "loss: 5.536560  [159010/315099]\n",
      "loss: 5.524693  [159260/315099]\n",
      "loss: 5.693731  [159510/315099]\n",
      "loss: 5.891746  [159760/315099]\n",
      "loss: 5.498006  [160010/315099]\n",
      "loss: 6.038145  [160260/315099]\n",
      "loss: 5.759886  [160510/315099]\n",
      "loss: 5.894553  [160760/315099]\n",
      "loss: 4.906087  [161010/315099]\n",
      "loss: 5.771281  [161260/315099]\n",
      "loss: 6.817445  [161510/315099]\n",
      "loss: 5.855214  [161760/315099]\n",
      "loss: 6.669598  [162010/315099]\n",
      "loss: 4.829585  [162260/315099]\n",
      "loss: 6.788313  [162510/315099]\n",
      "loss: 5.545371  [162760/315099]\n",
      "loss: 6.647700  [163010/315099]\n",
      "loss: 4.908153  [163260/315099]\n",
      "loss: 7.317820  [163510/315099]\n",
      "loss: 6.405786  [163760/315099]\n",
      "loss: 6.612736  [164010/315099]\n",
      "loss: 5.600079  [164260/315099]\n",
      "loss: 5.699867  [164510/315099]\n",
      "loss: 6.255135  [164760/315099]\n",
      "loss: 5.118053  [165010/315099]\n",
      "loss: 5.035921  [165260/315099]\n",
      "loss: 6.521469  [165510/315099]\n",
      "loss: 5.616701  [165760/315099]\n",
      "loss: 5.270495  [166010/315099]\n",
      "loss: 5.521083  [166260/315099]\n",
      "loss: 6.079265  [166510/315099]\n",
      "loss: 5.167435  [166760/315099]\n",
      "loss: 6.691395  [167010/315099]\n",
      "loss: 5.920499  [167260/315099]\n",
      "loss: 6.570325  [167510/315099]\n",
      "loss: 5.222001  [167760/315099]\n",
      "loss: 4.695475  [168010/315099]\n",
      "loss: 6.356529  [168260/315099]\n",
      "loss: 6.116768  [168510/315099]\n",
      "loss: 5.690196  [168760/315099]\n",
      "loss: 5.683904  [169010/315099]\n",
      "loss: 6.384341  [169260/315099]\n",
      "loss: 5.094924  [169510/315099]\n",
      "loss: 5.857692  [169760/315099]\n",
      "loss: 6.047852  [170010/315099]\n",
      "loss: 5.548035  [170260/315099]\n",
      "loss: 5.054994  [170510/315099]\n",
      "loss: 5.788365  [170760/315099]\n",
      "loss: 6.531385  [171010/315099]\n",
      "loss: 6.095771  [171260/315099]\n",
      "loss: 5.476252  [171510/315099]\n",
      "loss: 5.905184  [171760/315099]\n",
      "loss: 7.044806  [172010/315099]\n",
      "loss: 5.737127  [172260/315099]\n",
      "loss: 5.987885  [172510/315099]\n",
      "loss: 5.366968  [172760/315099]\n",
      "loss: 6.249268  [173010/315099]\n",
      "loss: 4.807412  [173260/315099]\n",
      "loss: 5.404614  [173510/315099]\n",
      "loss: 7.096686  [173760/315099]\n",
      "loss: 4.768262  [174010/315099]\n",
      "loss: 6.114099  [174260/315099]\n",
      "loss: 6.675318  [174510/315099]\n",
      "loss: 5.589540  [174760/315099]\n",
      "loss: 5.895062  [175010/315099]\n",
      "loss: 6.393035  [175260/315099]\n",
      "loss: 5.757105  [175510/315099]\n",
      "loss: 5.885561  [175760/315099]\n",
      "loss: 6.058896  [176010/315099]\n",
      "loss: 5.282695  [176260/315099]\n",
      "loss: 4.860107  [176510/315099]\n",
      "loss: 6.061532  [176760/315099]\n",
      "loss: 5.767953  [177010/315099]\n",
      "loss: 6.942347  [177260/315099]\n",
      "loss: 5.748030  [177510/315099]\n",
      "loss: 5.306682  [177760/315099]\n",
      "loss: 4.993019  [178010/315099]\n",
      "loss: 7.214492  [178260/315099]\n",
      "loss: 7.227545  [178510/315099]\n",
      "loss: 7.172908  [178760/315099]\n",
      "loss: 6.320110  [179010/315099]\n",
      "loss: 6.462505  [179260/315099]\n",
      "loss: 5.285375  [179510/315099]\n",
      "loss: 5.859394  [179760/315099]\n",
      "loss: 6.171483  [180010/315099]\n",
      "loss: 5.701818  [180260/315099]\n",
      "loss: 5.493955  [180510/315099]\n",
      "loss: 5.864417  [180760/315099]\n",
      "loss: 5.926420  [181010/315099]\n",
      "loss: 5.326053  [181260/315099]\n",
      "loss: 5.563619  [181510/315099]\n",
      "loss: 7.199750  [181760/315099]\n",
      "loss: 5.614472  [182010/315099]\n",
      "loss: 5.642159  [182260/315099]\n",
      "loss: 5.746092  [182510/315099]\n",
      "loss: 4.837262  [182760/315099]\n",
      "loss: 6.530595  [183010/315099]\n",
      "loss: 6.777446  [183260/315099]\n",
      "loss: 5.877481  [183510/315099]\n",
      "loss: 5.105739  [183760/315099]\n",
      "loss: 5.071005  [184010/315099]\n",
      "loss: 5.038604  [184260/315099]\n",
      "loss: 5.653432  [184510/315099]\n",
      "loss: 5.836074  [184760/315099]\n",
      "loss: 6.280052  [185010/315099]\n",
      "loss: 5.928045  [185260/315099]\n",
      "loss: 7.443349  [185510/315099]\n",
      "loss: 5.437991  [185760/315099]\n",
      "loss: 6.618697  [186010/315099]\n",
      "loss: 6.350614  [186260/315099]\n",
      "loss: 6.271775  [186510/315099]\n",
      "loss: 5.076193  [186760/315099]\n",
      "loss: 6.428864  [187010/315099]\n",
      "loss: 5.870500  [187260/315099]\n",
      "loss: 6.678442  [187510/315099]\n",
      "loss: 6.651372  [187760/315099]\n",
      "loss: 5.724319  [188010/315099]\n",
      "loss: 5.865261  [188260/315099]\n",
      "loss: 5.484859  [188510/315099]\n",
      "loss: 5.399481  [188760/315099]\n",
      "loss: 5.646819  [189010/315099]\n",
      "loss: 6.136851  [189260/315099]\n",
      "loss: 6.563828  [189510/315099]\n",
      "loss: 6.359658  [189760/315099]\n",
      "loss: 5.228701  [190010/315099]\n",
      "loss: 5.138406  [190260/315099]\n",
      "loss: 5.402923  [190510/315099]\n",
      "loss: 6.776453  [190760/315099]\n",
      "loss: 4.514982  [191010/315099]\n",
      "loss: 5.896426  [191260/315099]\n",
      "loss: 6.472717  [191510/315099]\n",
      "loss: 5.508912  [191760/315099]\n",
      "loss: 5.504745  [192010/315099]\n",
      "loss: 6.946058  [192260/315099]\n",
      "loss: 6.052213  [192510/315099]\n",
      "loss: 5.603763  [192760/315099]\n",
      "loss: 6.686999  [193010/315099]\n",
      "loss: 6.581075  [193260/315099]\n",
      "loss: 5.339744  [193510/315099]\n",
      "loss: 5.874573  [193760/315099]\n",
      "loss: 5.721694  [194010/315099]\n",
      "loss: 6.351838  [194260/315099]\n",
      "loss: 7.106908  [194510/315099]\n",
      "loss: 5.195096  [194760/315099]\n",
      "loss: 5.726705  [195010/315099]\n",
      "loss: 5.830409  [195260/315099]\n",
      "loss: 5.522908  [195510/315099]\n",
      "loss: 6.390098  [195760/315099]\n",
      "loss: 5.057020  [196010/315099]\n",
      "loss: 5.284175  [196260/315099]\n",
      "loss: 6.039053  [196510/315099]\n",
      "loss: 5.160885  [196760/315099]\n",
      "loss: 5.621149  [197010/315099]\n",
      "loss: 6.742558  [197260/315099]\n",
      "loss: 6.894149  [197510/315099]\n",
      "loss: 6.328892  [197760/315099]\n",
      "loss: 6.992403  [198010/315099]\n",
      "loss: 6.735143  [198260/315099]\n",
      "loss: 6.643561  [198510/315099]\n",
      "loss: 5.654455  [198760/315099]\n",
      "loss: 5.913422  [199010/315099]\n",
      "loss: 6.205110  [199260/315099]\n",
      "loss: 6.166024  [199510/315099]\n",
      "loss: 6.853922  [199760/315099]\n",
      "loss: 4.887305  [200010/315099]\n",
      "loss: 5.015211  [200260/315099]\n",
      "loss: 4.943728  [200510/315099]\n",
      "loss: 5.691796  [200760/315099]\n",
      "loss: 5.143955  [201010/315099]\n",
      "loss: 5.698812  [201260/315099]\n",
      "loss: 5.982134  [201510/315099]\n",
      "loss: 5.858615  [201760/315099]\n",
      "loss: 5.860550  [202010/315099]\n",
      "loss: 5.135852  [202260/315099]\n",
      "loss: 6.144855  [202510/315099]\n",
      "loss: 5.432499  [202760/315099]\n",
      "loss: 5.444621  [203010/315099]\n",
      "loss: 6.379045  [203260/315099]\n",
      "loss: 6.175747  [203510/315099]\n",
      "loss: 6.089386  [203760/315099]\n",
      "loss: 6.631608  [204010/315099]\n",
      "loss: 5.891299  [204260/315099]\n",
      "loss: 5.712766  [204510/315099]\n",
      "loss: 6.531053  [204760/315099]\n",
      "loss: 6.405281  [205010/315099]\n",
      "loss: 6.793043  [205260/315099]\n",
      "loss: 5.914554  [205510/315099]\n",
      "loss: 5.051553  [205760/315099]\n",
      "loss: 5.469918  [206010/315099]\n",
      "loss: 6.045856  [206260/315099]\n",
      "loss: 5.482566  [206510/315099]\n",
      "loss: 5.567135  [206760/315099]\n",
      "loss: 5.806179  [207010/315099]\n",
      "loss: 5.695700  [207260/315099]\n",
      "loss: 7.185231  [207510/315099]\n",
      "loss: 5.620177  [207760/315099]\n",
      "loss: 5.536965  [208010/315099]\n",
      "loss: 6.242403  [208260/315099]\n",
      "loss: 6.117156  [208510/315099]\n",
      "loss: 5.722831  [208760/315099]\n",
      "loss: 4.853578  [209010/315099]\n",
      "loss: 5.111483  [209260/315099]\n",
      "loss: 6.099254  [209510/315099]\n",
      "loss: 5.559920  [209760/315099]\n",
      "loss: 6.084441  [210010/315099]\n",
      "loss: 5.843957  [210260/315099]\n",
      "loss: 6.227767  [210510/315099]\n",
      "loss: 6.253803  [210760/315099]\n",
      "loss: 5.826365  [211010/315099]\n",
      "loss: 5.514542  [211260/315099]\n",
      "loss: 5.705348  [211510/315099]\n",
      "loss: 6.398528  [211760/315099]\n",
      "loss: 6.157653  [212010/315099]\n",
      "loss: 5.269744  [212260/315099]\n",
      "loss: 5.657246  [212510/315099]\n",
      "loss: 6.462678  [212760/315099]\n",
      "loss: 5.429337  [213010/315099]\n",
      "loss: 6.278729  [213260/315099]\n",
      "loss: 5.895277  [213510/315099]\n",
      "loss: 6.500802  [213760/315099]\n",
      "loss: 6.033208  [214010/315099]\n",
      "loss: 6.680216  [214260/315099]\n",
      "loss: 6.012305  [214510/315099]\n",
      "loss: 4.975585  [214760/315099]\n",
      "loss: 6.189352  [215010/315099]\n",
      "loss: 5.662011  [215260/315099]\n",
      "loss: 6.062052  [215510/315099]\n",
      "loss: 6.004844  [215760/315099]\n",
      "loss: 6.426159  [216010/315099]\n",
      "loss: 6.114411  [216260/315099]\n",
      "loss: 7.076178  [216510/315099]\n",
      "loss: 5.967170  [216760/315099]\n",
      "loss: 5.867771  [217010/315099]\n",
      "loss: 6.541875  [217260/315099]\n",
      "loss: 4.862122  [217510/315099]\n",
      "loss: 5.497530  [217760/315099]\n",
      "loss: 6.160246  [218010/315099]\n",
      "loss: 6.366198  [218260/315099]\n",
      "loss: 6.550494  [218510/315099]\n",
      "loss: 5.503500  [218760/315099]\n",
      "loss: 5.636589  [219010/315099]\n",
      "loss: 5.555314  [219260/315099]\n",
      "loss: 4.205261  [219510/315099]\n",
      "loss: 5.150405  [219760/315099]\n",
      "loss: 5.633761  [220010/315099]\n",
      "loss: 6.263443  [220260/315099]\n",
      "loss: 5.849387  [220510/315099]\n",
      "loss: 5.568628  [220760/315099]\n",
      "loss: 5.582467  [221010/315099]\n",
      "loss: 5.846682  [221260/315099]\n",
      "loss: 6.492166  [221510/315099]\n",
      "loss: 5.076818  [221760/315099]\n",
      "loss: 5.892263  [222010/315099]\n",
      "loss: 5.613853  [222260/315099]\n",
      "loss: 6.112224  [222510/315099]\n",
      "loss: 5.931668  [222760/315099]\n",
      "loss: 4.832134  [223010/315099]\n",
      "loss: 5.444696  [223260/315099]\n",
      "loss: 6.296514  [223510/315099]\n",
      "loss: 7.169262  [223760/315099]\n",
      "loss: 6.216761  [224010/315099]\n",
      "loss: 7.707651  [224260/315099]\n",
      "loss: 5.261980  [224510/315099]\n",
      "loss: 5.249741  [224760/315099]\n",
      "loss: 7.543623  [225010/315099]\n",
      "loss: 6.222361  [225260/315099]\n",
      "loss: 5.457996  [225510/315099]\n",
      "loss: 5.383966  [225760/315099]\n",
      "loss: 5.940668  [226010/315099]\n",
      "loss: 6.023719  [226260/315099]\n",
      "loss: 5.682658  [226510/315099]\n",
      "loss: 5.279673  [226760/315099]\n",
      "loss: 4.625941  [227010/315099]\n",
      "loss: 5.126915  [227260/315099]\n",
      "loss: 5.703351  [227510/315099]\n",
      "loss: 5.481549  [227760/315099]\n",
      "loss: 6.044705  [228010/315099]\n",
      "loss: 5.542962  [228260/315099]\n",
      "loss: 5.542489  [228510/315099]\n",
      "loss: 5.874898  [228760/315099]\n",
      "loss: 5.542457  [229010/315099]\n",
      "loss: 5.550695  [229260/315099]\n",
      "loss: 4.151533  [229510/315099]\n",
      "loss: 6.189655  [229760/315099]\n",
      "loss: 6.765874  [230010/315099]\n",
      "loss: 7.176406  [230260/315099]\n",
      "loss: 5.960963  [230510/315099]\n",
      "loss: 5.580102  [230760/315099]\n",
      "loss: 6.630153  [231010/315099]\n",
      "loss: 6.364152  [231260/315099]\n",
      "loss: 5.610952  [231510/315099]\n",
      "loss: 6.026783  [231760/315099]\n",
      "loss: 6.152970  [232010/315099]\n",
      "loss: 5.799375  [232260/315099]\n",
      "loss: 5.724225  [232510/315099]\n",
      "loss: 6.034964  [232760/315099]\n",
      "loss: 5.962392  [233010/315099]\n",
      "loss: 6.868066  [233260/315099]\n",
      "loss: 6.182170  [233510/315099]\n",
      "loss: 4.855830  [233760/315099]\n",
      "loss: 6.517137  [234010/315099]\n",
      "loss: 5.553771  [234260/315099]\n",
      "loss: 5.529459  [234510/315099]\n",
      "loss: 5.905644  [234760/315099]\n",
      "loss: 5.000227  [235010/315099]\n",
      "loss: 5.530410  [235260/315099]\n",
      "loss: 5.573562  [235510/315099]\n",
      "loss: 4.224981  [235760/315099]\n",
      "loss: 6.268182  [236010/315099]\n",
      "loss: 5.126056  [236260/315099]\n",
      "loss: 5.550523  [236510/315099]\n",
      "loss: 5.710222  [236760/315099]\n",
      "loss: 5.722848  [237010/315099]\n",
      "loss: 5.629778  [237260/315099]\n",
      "loss: 5.062337  [237510/315099]\n",
      "loss: 5.338885  [237760/315099]\n",
      "loss: 5.544201  [238010/315099]\n",
      "loss: 5.643241  [238260/315099]\n",
      "loss: 5.675958  [238510/315099]\n",
      "loss: 6.026664  [238760/315099]\n",
      "loss: 5.296287  [239010/315099]\n",
      "loss: 6.079097  [239260/315099]\n",
      "loss: 6.854454  [239510/315099]\n",
      "loss: 6.796161  [239760/315099]\n",
      "loss: 6.619112  [240010/315099]\n",
      "loss: 5.059497  [240260/315099]\n",
      "loss: 4.927001  [240510/315099]\n",
      "loss: 4.719588  [240760/315099]\n",
      "loss: 5.210097  [241010/315099]\n",
      "loss: 5.808639  [241260/315099]\n",
      "loss: 4.657155  [241510/315099]\n",
      "loss: 6.461756  [241760/315099]\n",
      "loss: 6.056880  [242010/315099]\n",
      "loss: 6.519758  [242260/315099]\n",
      "loss: 5.570744  [242510/315099]\n",
      "loss: 6.123762  [242760/315099]\n",
      "loss: 5.524493  [243010/315099]\n",
      "loss: 5.979208  [243260/315099]\n",
      "loss: 6.026179  [243510/315099]\n",
      "loss: 7.397696  [243760/315099]\n",
      "loss: 5.620703  [244010/315099]\n",
      "loss: 6.004555  [244260/315099]\n",
      "loss: 6.326797  [244510/315099]\n",
      "loss: 6.073988  [244760/315099]\n",
      "loss: 6.256528  [245010/315099]\n",
      "loss: 6.672004  [245260/315099]\n",
      "loss: 6.627952  [245510/315099]\n",
      "loss: 5.888719  [245760/315099]\n",
      "loss: 5.359448  [246010/315099]\n",
      "loss: 5.971531  [246260/315099]\n",
      "loss: 4.799785  [246510/315099]\n",
      "loss: 6.898352  [246760/315099]\n",
      "loss: 6.022816  [247010/315099]\n",
      "loss: 5.634982  [247260/315099]\n",
      "loss: 6.099343  [247510/315099]\n",
      "loss: 5.890924  [247760/315099]\n",
      "loss: 5.680448  [248010/315099]\n",
      "loss: 5.482331  [248260/315099]\n",
      "loss: 5.847688  [248510/315099]\n",
      "loss: 4.462606  [248760/315099]\n",
      "loss: 5.793257  [249010/315099]\n",
      "loss: 5.412385  [249260/315099]\n",
      "loss: 5.448211  [249510/315099]\n",
      "loss: 6.507512  [249760/315099]\n",
      "loss: 5.302157  [250010/315099]\n",
      "loss: 6.069054  [250260/315099]\n",
      "loss: 5.584935  [250510/315099]\n",
      "loss: 5.208978  [250760/315099]\n",
      "loss: 6.549973  [251010/315099]\n",
      "loss: 5.993894  [251260/315099]\n",
      "loss: 5.323632  [251510/315099]\n",
      "loss: 5.097573  [251760/315099]\n",
      "loss: 6.367716  [252010/315099]\n",
      "loss: 6.724463  [252260/315099]\n",
      "loss: 5.947637  [252510/315099]\n",
      "loss: 5.172493  [252760/315099]\n",
      "loss: 5.636313  [253010/315099]\n",
      "loss: 5.686598  [253260/315099]\n",
      "loss: 6.308385  [253510/315099]\n",
      "loss: 6.431106  [253760/315099]\n",
      "loss: 5.916325  [254010/315099]\n",
      "loss: 6.378079  [254260/315099]\n",
      "loss: 5.397529  [254510/315099]\n",
      "loss: 5.900576  [254760/315099]\n",
      "loss: 6.151320  [255010/315099]\n",
      "loss: 5.467947  [255260/315099]\n",
      "loss: 5.957474  [255510/315099]\n",
      "loss: 5.306574  [255760/315099]\n",
      "loss: 6.434674  [256010/315099]\n",
      "loss: 6.088781  [256260/315099]\n",
      "loss: 6.873358  [256510/315099]\n",
      "loss: 6.679532  [256760/315099]\n",
      "loss: 6.215873  [257010/315099]\n",
      "loss: 5.629449  [257260/315099]\n",
      "loss: 5.875531  [257510/315099]\n",
      "loss: 5.786007  [257760/315099]\n",
      "loss: 6.447911  [258010/315099]\n",
      "loss: 5.864378  [258260/315099]\n",
      "loss: 5.692430  [258510/315099]\n",
      "loss: 4.994004  [258760/315099]\n",
      "loss: 5.319076  [259010/315099]\n",
      "loss: 7.148890  [259260/315099]\n",
      "loss: 6.090774  [259510/315099]\n",
      "loss: 5.348240  [259760/315099]\n",
      "loss: 4.996878  [260010/315099]\n",
      "loss: 6.924541  [260260/315099]\n",
      "loss: 7.020978  [260510/315099]\n",
      "loss: 5.158395  [260760/315099]\n",
      "loss: 7.012452  [261010/315099]\n",
      "loss: 5.470738  [261260/315099]\n",
      "loss: 7.015225  [261510/315099]\n",
      "loss: 5.879066  [261760/315099]\n",
      "loss: 5.440637  [262010/315099]\n",
      "loss: 6.395955  [262260/315099]\n",
      "loss: 5.429152  [262510/315099]\n",
      "loss: 6.239184  [262760/315099]\n",
      "loss: 5.707485  [263010/315099]\n",
      "loss: 5.820235  [263260/315099]\n",
      "loss: 5.322025  [263510/315099]\n",
      "loss: 6.179879  [263760/315099]\n",
      "loss: 6.075029  [264010/315099]\n",
      "loss: 6.065502  [264260/315099]\n",
      "loss: 4.219081  [264510/315099]\n",
      "loss: 6.508399  [264760/315099]\n",
      "loss: 5.770905  [265010/315099]\n",
      "loss: 5.669015  [265260/315099]\n",
      "loss: 6.245439  [265510/315099]\n",
      "loss: 5.579519  [265760/315099]\n",
      "loss: 5.418947  [266010/315099]\n",
      "loss: 6.264884  [266260/315099]\n",
      "loss: 6.497813  [266510/315099]\n",
      "loss: 5.666178  [266760/315099]\n",
      "loss: 6.053158  [267010/315099]\n",
      "loss: 7.227223  [267260/315099]\n",
      "loss: 5.579057  [267510/315099]\n",
      "loss: 5.774649  [267760/315099]\n",
      "loss: 5.356189  [268010/315099]\n",
      "loss: 5.952511  [268260/315099]\n",
      "loss: 6.505071  [268510/315099]\n",
      "loss: 6.474085  [268760/315099]\n",
      "loss: 6.535532  [269010/315099]\n",
      "loss: 5.305902  [269260/315099]\n",
      "loss: 7.212275  [269510/315099]\n",
      "loss: 5.656618  [269760/315099]\n",
      "loss: 5.628150  [270010/315099]\n",
      "loss: 5.624639  [270260/315099]\n",
      "loss: 4.794053  [270510/315099]\n",
      "loss: 5.321870  [270760/315099]\n",
      "loss: 6.260240  [271010/315099]\n",
      "loss: 6.203224  [271260/315099]\n",
      "loss: 6.479626  [271510/315099]\n",
      "loss: 6.324145  [271760/315099]\n",
      "loss: 5.612194  [272010/315099]\n",
      "loss: 5.639466  [272260/315099]\n",
      "loss: 6.730799  [272510/315099]\n",
      "loss: 5.713291  [272760/315099]\n",
      "loss: 5.482506  [273010/315099]\n",
      "loss: 6.810484  [273260/315099]\n",
      "loss: 7.379701  [273510/315099]\n",
      "loss: 6.465689  [273760/315099]\n",
      "loss: 5.402886  [274010/315099]\n",
      "loss: 4.974177  [274260/315099]\n",
      "loss: 6.715196  [274510/315099]\n",
      "loss: 5.931772  [274760/315099]\n",
      "loss: 5.643807  [275010/315099]\n",
      "loss: 6.933864  [275260/315099]\n",
      "loss: 5.084899  [275510/315099]\n",
      "loss: 5.475577  [275760/315099]\n",
      "loss: 6.451669  [276010/315099]\n",
      "loss: 4.944589  [276260/315099]\n",
      "loss: 5.675569  [276510/315099]\n",
      "loss: 6.718169  [276760/315099]\n",
      "loss: 5.976273  [277010/315099]\n",
      "loss: 6.578902  [277260/315099]\n",
      "loss: 5.590810  [277510/315099]\n",
      "loss: 6.228886  [277760/315099]\n",
      "loss: 5.379634  [278010/315099]\n",
      "loss: 6.587259  [278260/315099]\n",
      "loss: 5.325395  [278510/315099]\n",
      "loss: 6.312713  [278760/315099]\n",
      "loss: 4.890890  [279010/315099]\n",
      "loss: 6.251590  [279260/315099]\n",
      "loss: 5.898847  [279510/315099]\n",
      "loss: 6.207988  [279760/315099]\n",
      "loss: 6.864610  [280010/315099]\n",
      "loss: 5.348273  [280260/315099]\n",
      "loss: 6.532248  [280510/315099]\n",
      "loss: 5.702218  [280760/315099]\n",
      "loss: 5.758342  [281010/315099]\n",
      "loss: 5.987152  [281260/315099]\n",
      "loss: 6.335354  [281510/315099]\n",
      "loss: 6.262598  [281760/315099]\n",
      "loss: 6.311282  [282010/315099]\n",
      "loss: 6.541702  [282260/315099]\n",
      "loss: 5.324957  [282510/315099]\n",
      "loss: 5.656531  [282760/315099]\n",
      "loss: 5.337378  [283010/315099]\n",
      "loss: 5.590827  [283260/315099]\n",
      "loss: 6.557242  [283510/315099]\n",
      "loss: 5.461510  [283760/315099]\n",
      "loss: 5.738515  [284010/315099]\n",
      "loss: 5.235838  [284260/315099]\n",
      "loss: 6.205856  [284510/315099]\n",
      "loss: 5.525204  [284760/315099]\n",
      "loss: 5.719166  [285010/315099]\n",
      "loss: 5.966422  [285260/315099]\n",
      "loss: 5.317856  [285510/315099]\n",
      "loss: 5.677148  [285760/315099]\n",
      "loss: 6.356651  [286010/315099]\n",
      "loss: 5.458831  [286260/315099]\n",
      "loss: 6.448419  [286510/315099]\n",
      "loss: 6.013592  [286760/315099]\n",
      "loss: 5.687392  [287010/315099]\n",
      "loss: 7.307912  [287260/315099]\n",
      "loss: 6.250408  [287510/315099]\n",
      "loss: 6.733978  [287760/315099]\n",
      "loss: 6.632370  [288010/315099]\n",
      "loss: 5.920227  [288260/315099]\n",
      "loss: 7.241007  [288510/315099]\n",
      "loss: 5.788527  [288760/315099]\n",
      "loss: 5.516295  [289010/315099]\n",
      "loss: 4.673616  [289260/315099]\n",
      "loss: 6.452878  [289510/315099]\n",
      "loss: 5.543840  [289760/315099]\n",
      "loss: 5.975137  [290010/315099]\n",
      "loss: 5.032279  [290260/315099]\n",
      "loss: 5.258048  [290510/315099]\n",
      "loss: 4.833169  [290760/315099]\n",
      "loss: 5.926924  [291010/315099]\n",
      "loss: 5.778735  [291260/315099]\n",
      "loss: 8.233810  [291510/315099]\n",
      "loss: 5.025638  [291760/315099]\n",
      "loss: 7.132861  [292010/315099]\n",
      "loss: 5.885512  [292260/315099]\n",
      "loss: 5.859645  [292510/315099]\n",
      "loss: 6.003551  [292760/315099]\n",
      "loss: 5.379457  [293010/315099]\n",
      "loss: 5.293828  [293260/315099]\n",
      "loss: 6.664470  [293510/315099]\n",
      "loss: 6.283136  [293760/315099]\n",
      "loss: 6.541751  [294010/315099]\n",
      "loss: 5.126401  [294260/315099]\n",
      "loss: 5.993542  [294510/315099]\n",
      "loss: 6.540353  [294760/315099]\n",
      "loss: 6.911031  [295010/315099]\n",
      "loss: 5.175021  [295260/315099]\n",
      "loss: 5.467577  [295510/315099]\n",
      "loss: 5.606902  [295760/315099]\n",
      "loss: 6.306511  [296010/315099]\n",
      "loss: 7.221705  [296260/315099]\n",
      "loss: 5.088583  [296510/315099]\n",
      "loss: 5.340347  [296760/315099]\n",
      "loss: 5.286562  [297010/315099]\n",
      "loss: 5.469578  [297260/315099]\n",
      "loss: 5.739929  [297510/315099]\n",
      "loss: 5.007974  [297760/315099]\n",
      "loss: 6.552901  [298010/315099]\n",
      "loss: 5.026207  [298260/315099]\n",
      "loss: 6.360312  [298510/315099]\n",
      "loss: 6.975237  [298760/315099]\n",
      "loss: 5.473813  [299010/315099]\n",
      "loss: 5.954525  [299260/315099]\n",
      "loss: 5.835425  [299510/315099]\n",
      "loss: 5.305673  [299760/315099]\n",
      "loss: 5.851717  [300010/315099]\n",
      "loss: 4.583344  [300260/315099]\n",
      "loss: 6.258643  [300510/315099]\n",
      "loss: 5.738436  [300760/315099]\n",
      "loss: 5.701629  [301010/315099]\n",
      "loss: 6.865172  [301260/315099]\n",
      "loss: 5.392505  [301510/315099]\n",
      "loss: 6.388428  [301760/315099]\n",
      "loss: 5.837920  [302010/315099]\n",
      "loss: 5.211386  [302260/315099]\n",
      "loss: 5.507857  [302510/315099]\n",
      "loss: 5.817996  [302760/315099]\n",
      "loss: 6.595485  [303010/315099]\n",
      "loss: 6.519748  [303260/315099]\n",
      "loss: 5.821724  [303510/315099]\n",
      "loss: 5.474336  [303760/315099]\n",
      "loss: 5.288499  [304010/315099]\n",
      "loss: 6.326548  [304260/315099]\n",
      "loss: 6.279850  [304510/315099]\n",
      "loss: 5.037138  [304760/315099]\n",
      "loss: 6.075569  [305010/315099]\n",
      "loss: 5.337338  [305260/315099]\n",
      "loss: 6.564440  [305510/315099]\n",
      "loss: 6.213698  [305760/315099]\n",
      "loss: 7.579240  [306010/315099]\n",
      "loss: 5.425990  [306260/315099]\n",
      "loss: 7.223590  [306510/315099]\n",
      "loss: 4.895824  [306760/315099]\n",
      "loss: 4.266957  [307010/315099]\n",
      "loss: 5.599019  [307260/315099]\n",
      "loss: 5.491933  [307510/315099]\n",
      "loss: 6.118319  [307760/315099]\n",
      "loss: 6.964824  [308010/315099]\n",
      "loss: 5.276421  [308260/315099]\n",
      "loss: 6.293175  [308510/315099]\n",
      "loss: 7.549560  [308760/315099]\n",
      "loss: 4.913933  [309010/315099]\n",
      "loss: 6.901267  [309260/315099]\n",
      "loss: 6.129705  [309510/315099]\n",
      "loss: 4.853433  [309760/315099]\n",
      "loss: 6.816947  [310010/315099]\n",
      "loss: 6.915853  [310260/315099]\n",
      "loss: 4.841729  [310510/315099]\n",
      "loss: 6.068507  [310760/315099]\n",
      "loss: 6.565633  [311010/315099]\n",
      "loss: 6.066830  [311260/315099]\n",
      "loss: 6.853619  [311510/315099]\n",
      "loss: 5.431708  [311760/315099]\n",
      "loss: 5.502876  [312010/315099]\n",
      "loss: 5.612552  [312260/315099]\n",
      "loss: 6.109493  [312510/315099]\n",
      "loss: 5.456954  [312760/315099]\n",
      "loss: 6.827365  [313010/315099]\n",
      "loss: 5.891810  [313260/315099]\n",
      "loss: 6.114574  [313510/315099]\n",
      "loss: 5.568515  [313760/315099]\n",
      "loss: 5.916656  [314010/315099]\n",
      "loss: 5.549307  [314260/315099]\n",
      "loss: 5.658634  [314510/315099]\n",
      "loss: 4.885008  [314760/315099]\n",
      "loss: 5.031166  [315010/315099]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "learning_rate = 0.0001\n",
    "batch_size = 10\n",
    "\n",
    "tokenixer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with open(\"sample-quij.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        contenido = file.read()\n",
    "train_dataset = text_dataset(contenido, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "model = NeuralNetwork(tokenizer.vocab_size).to(\"cuda\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fda39494-75d5-4868-8fca-3e663a426563",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'encoder.pth')\n",
    "def use_model(model, frase_input,tokenizer, context_size=50):\n",
    "    frase = tokenizer.encode(frase_input)\n",
    "    frase = frase[1:-1]\n",
    "    \n",
    "    for i in range(context_size-len(frase)):\n",
    "        frase_zeros = frase.copy()\n",
    "        for i in range(context_size-len(frase)):\n",
    "            frase_zeros.append(0)\n",
    "        frase_zeros = torch.tensor(frase_zeros).reshape(1,50).to(\"cuda\")\n",
    "        logits = model(frase_zeros)\n",
    "        y = torch.argmax(logits, dim=1)\n",
    "        frase += y\n",
    "    print(tokenizer.decode(frase))\n",
    "    return frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c421dd-9991-4f60-b678-907dfe47feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,, que que, que que que, que que que que, que que que, que, que que que, que,, que que que que, que, que que que, que, que,, que, que,,,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(10861, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0'),\n",
       " tensor(1010, device='cuda:0')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = \"\"\n",
    "use_model(model, frase, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ab366-9bb6-4661-ba49-bbd5f67599be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2721])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64035e42-7de2-4638-905d-b9fdd5798950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLenguage",
   "language": "python",
   "name": "natural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
